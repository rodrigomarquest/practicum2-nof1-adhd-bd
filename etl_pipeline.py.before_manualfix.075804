# -*- coding: utf-8 -*-
from __future__ import annotations

import os, json, tempfile, hashlib

def _sha256_file(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024*1024), b""):
            h.update(chunk)
    return h.hexdigest()

def _write_atomic_csv(df, out_path: str):
    d = os.path.dirname(out_path) or "."
    fd, tmp = tempfile.mkstemp(prefix=".tmp_", dir=d, suffix=".csv")
    os.close(fd)
    try:
        df__CSV__tmp, index=False\)
        os.replace(tmp, out_path)
    finally:
        if os.path.exists(tmp):
            try: os.remove(tmp)
            except: pass

def _write_atomic_json(obj, out_path: str):
    d = os.path.dirname(out_path) or "."
    fd, tmp = tempfile.mkstemp(prefix=".tmp_", dir=d, suffix=".json")
    os.close(fd)
    try:
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
        os.replace(tmp, out_path)
    finally:
        if os.path.exists(tmp):
            try: os.remove(tmp)
            except: pass
"""
etl_pipeline.py — Nova arquitetura (definitivo)
------------------------------------------------
Subcomandos (no mesmo arquivo, sem orquestradores externos):
  extract : Apple → per-metric a partir de export.xml (minuto-a-minuto)
  cardio  : estágio Cardiovascular (consome per-metric + Zepp, grava features)
  full    : extract → cardio (pipeline novo ponta-a-ponta)

Saídas por snapshot (data_ai/<PID>/snapshots/<YYYY-MM-DD>/):
  per-metric/apple_heart_rate.csv        (timestamp,bpm)
  per-metric/apple_hrv_sdnn.csv          (timestamp,sdnn_ms)
  per-metric/apple_sleep_intervals.csv   (start,end,raw_value) [opcional]
  version_raw.csv
  version_log_enriched.csv               (segmentos start/end)
  features_cardiovascular.csv
  features_daily_updated.csv
"""
import argparse
import re
import sys
from dataclasses import dataclass
from datetime import datetime, date
from pathlib import Path
from typing import Optional, Iterable, Tuple, Any, Dict

import numpy as np
import pandas as pd

# --- Raízes canônicas ---
RAW_ROOT = Path("data_etl")
AI_ROOT  = Path("data_ai")

# --- Snapshots ---
_SNAP_ISO  = re.compile(r"^20\d{2}-[01]\d-[0-3]\d$")
_SNAP_8DIG = re.compile(r"^20\d{6}$")  # YYYYMMDD

def canon_snap_id(name: str) -> str:
    name = name.strip()
    if _SNAP_ISO.match(name): return name
    if _SNAP_8DIG.match(name): return f"{name[:4]}-{name[4:6]}-{name[6:8]}"
    raise ValueError(f"Invalid snapshot '{name}'. Expected YYYY-MM-DD or YYYYMMDD.")

def ensure_ai_outdir(pid: str, snap: str) -> Path:
    snap_iso = canon_snap_id(snap)
    outdir = AI_ROOT / pid / "snapshots" / snap_iso
    outdir.mkdir(parents=True, exist_ok=True)
    return outdir

def find_export_xml(pid: str, snap: str) -> Optional[Path]:
    snap_iso = canon_snap_id(snap)
    cands = [
        RAW_ROOT / pid / "snapshots" / snap_iso / "export.xml",
        RAW_ROOT / pid / "snapshots" / snap_iso.replace("-", "") / "export.xml",
        RAW_ROOT / pid / snap_iso / "export.xml",
        RAW_ROOT / pid / snap_iso.replace("-", "") / "export.xml",
    ]
    for p in cands:
        if p.exists(): return p
    return None

# ---------- Timezone (zoneinfo, fallback pytz) ----------
try:
    from zoneinfo import ZoneInfo  # py3.9+
except Exception:
    ZoneInfo = None
    try:
        import pytz  # type: ignore
    except Exception:
        pytz = None

def get_tz(tz_name: str):
    if ZoneInfo is not None:
        return ZoneInfo(tz_name)
    if 'pytz' in sys.modules or 'pytz' in globals():
        return pytz.timezone(tz_name)  # type: ignore
    raise RuntimeError("No timezone support. Use Python 3.9+ or install pytz.")

# ---------- HealthKit parsing (copiado/adaptado do legado) ----------
TYPE_MAP = {
    "HKQuantityTypeIdentifierHeartRate": ("hr", "count/min"),
    "HKQuantityTypeIdentifierHeartRateVariabilitySDNN": ("hrv_sdnn", "ms"),
    "HKCategoryTypeIdentifierSleepAnalysis": ("sleep", "category"),
    "HKQuantityTypeIdentifierBodyTemperature": ("temp", "degC"),
    "HKQuantityTypeIdentifierAppleSleepingWristTemperature": ("temp", "degC"),
    "HKQuantityTypeIdentifierBasalBodyTemperature": ("temp", "degC"),
}

ASLEEP_STR = {
    "HKCategoryValueSleepAnalysisAsleep",
    "HKCategoryValueSleepAnalysisAsleepUnspecified",
    "HKCategoryValueSleepAnalysisAsleepCore",
    "HKCategoryValueSleepAnalysisAsleepDeep",
    "HKCategoryValueSleepAnalysisAsleepREM",
}
ASLEEP_INT = {1, 3, 4, 5}

DEVICE_KEY_MAP = {"name":"name", "manufacturer":"manufacturer",
                  "model":"watch_model", "hardware":"watch_fw", "software":"ios_version"}

def fix_iso_tz(s: str) -> str:
    if s.endswith("Z"): return s[:-1] + "+00:00"
    m = re.search(r"([+-]\d{2})(\d{2})$", s)
    if m: s = s[:m.start()] + f"{m.group(1)}:{m.group(2)}"
    return s

def parse_dt(s: str) -> datetime:
    return datetime.fromisoformat(fix_iso_tz(s))

def parse_device_string(device_str: str) -> Dict[str, str]:
    info: Dict[str, str] = {}
    if not device_str: return info
    parts = [p.strip() for p in device_str.strip("<>").split(",")]
    for part in parts:
        if ":" in part:
            k, v = part.split(":", 1)
            info[k.strip().lower()] = v.strip()
    mapped: Dict[str, str] = {}
    for k, v in info.items():
        if k in DEVICE_KEY_MAP:
            mapped[DEVICE_KEY_MAP[k]] = v
    return mapped

def make_tz_selector(cutover: date, tz_before: str, tz_after: str):
    tz_b = get_tz(tz_before)
    tz_a = get_tz(tz_after)
    def selector(dt_aware: datetime):
        day_utc = dt_aware.astimezone(get_tz("UTC")).date()
        return tz_a if day_utc >= cutover else tz_b
    return selector

def iter_health_records(xml_path: Path, tz_selector) -> Iterable[Tuple[str, Any, datetime, Optional[datetime], str, Dict[str,str], str, str, str]]:
    import xml.etree.ElementTree as ET
    context = ET.iterparse(xml_path, events=("end",))
    for _, elem in context:
        if elem.tag != "Record":
            continue
        type_id = elem.attrib.get("type")
        if type_id not in TYPE_MAP:
            elem.clear(); continue

        s = elem.attrib.get("startDate")
        e = elem.attrib.get("endDate")
        if not s:
            elem.clear(); continue

        sdt = parse_dt(s)
        edt = parse_dt(e) if e else None

        tz = tz_selector(sdt)
        s_local = sdt.astimezone(tz)
        e_local = edt.astimezone(tz) if edt else None

        unit = elem.attrib.get("unit")
        val_raw = elem.attrib.get("value")

        if type_id == "HKCategoryTypeIdentifierSleepAnalysis":
            value = val_raw
        else:
            try:
                value = float(val_raw) if val_raw is not None else None
            except Exception:
                value = None

        device = parse_device_string(elem.attrib.get("device", ""))
        tz_name = getattr(tz, 'key', getattr(tz, 'zone', str(tz)))
        source_version = elem.attrib.get("sourceVersion", "")
        source_name    = elem.attrib.get("sourceName", "")

        yield type_id, value, s_local, e_local, unit, device, tz_name, source_version, source_name
        elem.clear()

# ---------- Segments ----------
def build_segments_from_versions(vdf: pd.DataFrame) -> pd.DataFrame:
    if vdf.empty:
        return pd.DataFrame(columns=["segment_id","start","end","ios_version","watch_model","watch_fw","tz_name","source_name"])
    df = vdf.copy()
    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date")
    keys = ["ios_version","watch_model","watch_fw","tz_name","source_name"]
    sig = df[keys].astype(str).agg(" | ".join, axis=1)
    change = sig.ne(sig.shift(fill_value="__START__"))
    segments = []
    seg_id = 0
    seg_start = df["date"].iloc[0]
    last_vals = df.loc[df.index[0], keys].to_dict()
    for i in range(1, len(df)):
        if change.iloc[i]:
            seg_id += 1
            segments.append({
                "segment_id": seg_id,
                "start": seg_start.date(),
                "end": df["date"].iloc[i-1].date(),
                **last_vals
            })
            seg_start = df["date"].iloc[i]
            last_vals = df.loc[df.index[i], keys].to_dict()
    seg_id += 1
    segments.append({
        "segment_id": seg_id,
        "start": seg_start.date(),
        "end": df["date"].iloc[-1].date(),
        **last_vals
    })
    return pd.DataFrame(segments)

# ---------- Apple → per-metric ----------
def extract_apple_per_metric(xml_file: Path, outdir: Path, cutover_str: str, tz_before: str, tz_after: str) -> Dict[str,str]:
    out_pm = outdir / "per-metric"
    out_pm.mkdir(parents=True, exist_ok=True)

    y, m, d = map(int, cutover_str.split("-"))
    tz_selector = make_tz_selector(date(y, m, d), tz_before, tz_after)

    records = iter_health_records(xml_file, tz_selector)

    hr_rows, hrv_rows, sleep_rows, version_rows = [], [], [], []
    HR_ID   = "HKQuantityTypeIdentifierHeartRate"
    HRV_ID  = "HKQuantityTypeIdentifierHeartRateVariabilitySDNN"
    SLEEP_ID= "HKCategoryTypeIdentifierSleepAnalysis"

    for (type_id, value, s_local, e_local, unit,
         device, tz_name, src_ver, src_name) in records:

        day = s_local.date()
        ios_ver = device.get("ios_version", "") or src_ver
        if ios_ver or device:
            version_rows.append({
                "date": day,
                "ios_version": ios_ver,
                "watch_model": device.get("watch_model", ""),
                "watch_fw": device.get("watch_fw", ""),
                "tz_name": tz_name,
                "source_name": src_name
            })

        if type_id == HR_ID and value is not None:
            hr_rows.append({"timestamp": s_local.isoformat(), "bpm": float(value)})
        elif type_id == HRV_ID and value is not None:
            hrv_rows.append({"timestamp": s_local.isoformat(), "sdnn_ms": float(value)})
        elif type_id == SLEEP_ID and e_local is not None:
            sleep_rows.append({"start": s_local.isoformat(), "end": e_local.isoformat(), "raw_value": value})

    written: Dict[str,str] = {}

    if hr_rows:
        p = out_pm / "apple_heart_rate.csv"
        pd.DataFrame(hr_rows).sort_values("timestamp")__CSV__p, index=False\)
        written["apple_heart_rate"] = str(p)
    if hrv_rows:
        p = out_pm / "apple_hrv_sdnn.csv"
        pd.DataFrame(hrv_rows).sort_values("timestamp")__CSV__p, index=False\)
        written["apple_hrv_sdnn"] = str(p)
    if sleep_rows:
        p = out_pm / "apple_sleep_intervals.csv"
        pd.DataFrame(sleep_rows).sort_values("start")__CSV__p, index=False\)
        written["apple_sleep_intervals"] = str(p)

    # version logs
    vdf = pd.DataFrame(version_rows).drop_duplicates().sort_values("date") if version_rows else pd.DataFrame()
    if not vdf.empty:
        vdf__CSV__outdir / "version_raw.csv", index=False\)
        seg_df = build_segments_from_versions(vdf)
        seg_df__CSV__outdir / "version_log_enriched.csv", index=False\)
        written["version_raw"] = str(outdir / "version_raw.csv")
        written["version_log_enriched"] = str(outdir / "version_log_enriched.csv")
    else:
        # ainda escrevemos arquivos vazios para consistência
        pd.DataFrame(columns=["date","ios_version","watch_model","watch_fw","tz_name","source_name"])__CSV__outdir / "version_raw.csv", index=False\)
        pd.DataFrame(columns=["segment_id","start","end","ios_version","watch_model","watch_fw","tz_name","source_name"])__CSV__outdir / "version_log_enriched.csv", index=False\)

    
    # ---- manifest com hashes do export.xml e saídas ----
    manifest = {
        "type": "extract",
        "export_xml": str(xml_file),
        "params": {"cutover": cutover_str, "tz_before": tz_before, "tz_after": tz_after},
        "outputs": {},
    }
    try:
        import os
        if os.path.exists(xml_file):
            manifest["export_sha256"] = _sha256_file(str(xml_file))
    except Exception:
        pass
    for k, v in list(written.items()):
        try:
            manifest["outputs"][k] = {"path": v, "sha256": _sha256_file(v)}
        except Exception:
            manifest["outputs"][k] = {"path": v, "sha256": None}
    _write_atomic_json(manifest, str(outdir / "extract_manifest.json"))
return written

# ---------- Cardiovascular stage ----------
def run_stage_cardio(snapshot_dir: Path) -> Dict[str,str]:
    from etl_modules.config import CardioCfg
    from etl_modules.cardiovascular.cardio_etl import run_stage_cardio as _run
    cfg = CardioCfg()
    res = _run(str(snapshot_dir), str(snapshot_dir), cfg)
    return {k: str(v) for k, v in res.items()}

# ---------- CLI ----------
def main():
    ap = argparse.ArgumentParser(description="ETL Apple/Zepp — Nova arquitetura (definitivo)")
    sub = ap.add_subparsers(dest="cmd", required=True)

    p_ext = sub.add_parser("extract", help="Apple → per-metric (a partir de export.xml)")
    p_ext.add_argument("--participant", required=True)
    p_ext.add_argument("--snapshot", required=True)
    p_ext.add_argument("--cutover", required=True)
    p_ext.add_argument("--tz_before", required=True)
    p_ext.add_argument("--tz_after", required=True)

    p_car = sub.add_parser("cardio", help="Executa estágio Cardiovascular no snapshot")
    p_car.add_argument("--zepp_dir", help="Diretório fixo com CSVs processados do Zepp (evita _latest)")
    g = p_car.add_mutually_exclusive_group(required=True)
    g.add_argument("--snapshot_dir", help="Caminho data_ai/<PID>/snapshots/<YYYY-MM-DD>")
    g.add_argument("--participant", help="PID (usar com --snapshot)")
    p_car.add_argument("--snapshot", help="Snapshot id (YYYY-MM-DD ou YYYYMMDD)")

    p_full = sub.add_parser("full", help="extract → cardio (pipeline novo ponta-a-ponta)")
    p_full.add_argument("--participant", required=True)
    p_full.add_argument("--snapshot", required=True)
    p_full.add_argument("--cutover", required=True)
    p_full.add_argument("--tz_before", required=True)
    p_full.add_argument("--tz_after", required=True)

    args = ap.parse_args()

    if args.cmd == "extract":
        pid, snap = args.participant, args.snapshot
        xml = find_export_xml(pid, snap)
        if xml is None:
            print("⚠️ export.xml não encontrado para:", pid, snap)
            return 1
        outdir = ensure_ai_outdir(pid, snap)
        written = extract_apple_per_metric(xml_file=xml, outdir=outdir,
                                           cutover_str=args.cutover,
                                           tz_before=args.tz_before, tz_after=args.tz_after)
        print("✅ extract concluído")
        for k,v in written.items():
            print(f" - {k}: {v}")
        return 0

    if args.cmd == "cardio":
        if args.snapshot_dir:
            snapdir = Path(args.snapshot_dir)
        else:
            if not args.participant or not args.snapshot:
                print("Use --snapshot_dir OU (--participant e --snapshot).")
                return 2
            snapdir = ensure_ai_outdir(args.participant, args.snapshot)
        if not snapdir.exists():
            print("⚠️ snapshot_dir não existe:", snapdir)
            return 1
        import os, glob
        if args.zepp_dir:
            os.environ["ZEPPOVERRIDE_DIR"] = args.zepp_dir
        res = run_stage_cardio(snapdir)

        # manifest do cardio (inputs e outputs com hashes)
        inputs = {}
        # per-metric
        for pm in ["apple_heart_rate.csv","apple_hrv_sdnn.csv","apple_sleep_intervals.csv"]:
            pth = snapdir / "per-metric" / pm
            if pth.exists():
                try:
                    inputs[pm] = {"path": str(pth), "sha256": _sha256_file(str(pth))}
                except Exception:
                    inputs[pm] = {"path": str(pth), "sha256": None}
        # segments
        for name in ["version_log_enriched.csv","version_raw.csv"]:
            pth = snapdir / name
            if pth.exists():
                try:
                    inputs[name] = {"path": str(pth), "sha256": _sha256_file(str(pth))}
                except Exception:
                    inputs[name] = {"path": str(pth), "sha256": None}
        # Zepp
        zdir = os.environ.get("ZEPPOVERRIDE_DIR","")
        if zdir:
            zepp_files = glob.glob(os.path.join(zdir, "*.csv"))
        else:
            zepp_files = []
            # manter compat: se não passar zepp_dir, o loader decidirá o caminho
        zepp = {}
        for z in zepp_files:
            try: zepp[z] = {"path": z, "sha256": _sha256_file(z)}
            except Exception: zepp[z] = {"path": z, "sha256": None}

        outputs = {}
        for k,v in res.items():
            try:
                outputs[k] = {"path": v, "sha256": _sha256_file(v)}
            except Exception:
                outputs[k] = {"path": v, "sha256": None}

        manifest = {
            "type": "cardio",
            "snapshot_dir": str(snapdir),
            "zepp_dir": zdir or None,
            "inputs": {"per_metric": inputs, "zepp": zepp},
            "outputs": outputs
        }
        _write_atomic_json(manifest, str(snapdir / "cardio_manifest.json"))

        print("✅ cardio concluído")
        print(res)
        return 0

    if args.cmd == "full":
        pid, snap = args.participant, args.snapshot
        xml = find_export_xml(pid, snap)
        if xml is None:
            print("⚠️ export.xml não encontrado para:", pid, snap)
            return 1
        outdir = ensure_ai_outdir(pid, snap)
        written = extract_apple_per_metric(xml_file=xml, outdir=outdir,
                                           cutover_str=args.cutover,
                                           tz_before=args.tz_before, tz_after=args.tz_after)
        print("✅ extract concluído")
        for k,v in written.items():
            print(f" - {k}: {v}")
        res = run_stage_cardio(outdir)
        print("✅ cardio concluído")
        print(res)
        return 0

    return 0

if __name__ == "__main__":
    sys.exit(main())

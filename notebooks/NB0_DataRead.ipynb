{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0ae5eb",
   "metadata": {},
   "source": [
    "# NB0: Data Readiness & Pipeline Stage Detection\n",
    "\n",
    "**Purpose**: Quickly verify if the necessary data for the N-of-1 pipeline are present and infer which ETL stages have been executed for P000001.\n",
    "\n",
    "**Pipeline**: practicum2-nof1-adhd-bd v4.1.x  \n",
    "**Participant**: P000001  \n",
    "**Snapshot**: 2025-11-07\n",
    "\n",
    "This notebook is a sanity check / onboarding tool that:\n",
    "1. Detects presence of key directories and files\n",
    "2. Maps these to pipeline stages (0-9)\n",
    "3. Provides actionable hints for missing stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39c28797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: c:\\dev\\practicum2-nof1-adhd-bd\n",
      "Checking data for: P000001 / 2025-11-07\n",
      "Data directory: c:\\dev\\practicum2-nof1-adhd-bd\\data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Configuration\n",
    "PARTICIPANT = \"P000001\"\n",
    "SNAPSHOT = \"2025-11-07\"\n",
    "\n",
    "# REPO_ROOT is one level up from notebooks/\n",
    "REPO_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Checking data for: {PARTICIPANT} / {SNAPSHOT}\")\n",
    "print(f\"Data directory: {REPO_ROOT / 'data'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5091b",
   "metadata": {},
   "source": [
    "## Stage Detection Logic\n",
    "\n",
    "Pipeline stages (from `scripts/run_full_pipeline.py`):\n",
    "\n",
    "- **Stage 0**: Ingest (extract from data/raw)\n",
    "- **Stage 1**: Aggregate (XML + CSVs to daily_*.csv)\n",
    "- **Stage 2**: Unify (merge daily CSVs)\n",
    "- **Stage 3**: Label (apply PBSI labels)\n",
    "- **Stage 4**: Segment (create segment_autolog.csv)\n",
    "- **Stage 5**: Prep NB2 (anti-leak dataset)\n",
    "- **Stage 6**: NB2 (baseline models)\n",
    "- **Stage 7**: NB3 (LSTM sequence models)\n",
    "- **Stage 8**: TFLite (mobile deployment)\n",
    "- **Stage 9**: Report (RUN_REPORT.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d043087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Stage detection logic configured\n"
     ]
    }
   ],
   "source": [
    "def check_file_exists(path: Path) -> bool:\n",
    "    \"\"\"Check if file exists and is non-empty.\"\"\"\n",
    "    return path.exists() and (path.is_dir() or path.stat().st_size > 0)\n",
    "\n",
    "def check_directory_has_files(path: Path, pattern: str = \"*\") -> bool:\n",
    "    \"\"\"Check if directory exists and contains files matching pattern.\"\"\"\n",
    "    if not path.exists() or not path.is_dir():\n",
    "        return False\n",
    "    return len(list(path.glob(pattern))) > 0\n",
    "\n",
    "# Define paths\n",
    "paths = {\n",
    "    \"raw_base\": REPO_ROOT / \"data\" / \"raw\",\n",
    "    \"etl_base\": REPO_ROOT / \"data\" / \"etl\" / PARTICIPANT / SNAPSHOT,\n",
    "    \"ai_base\": REPO_ROOT / \"data\" / \"ai\" / PARTICIPANT / SNAPSHOT,\n",
    "}\n",
    "\n",
    "# Stage-specific checks\n",
    "stage_checks = [\n",
    "    {\n",
    "        \"stage\": \"Stage 0: Ingest\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"zepp\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"apple_health_export\",\n",
    "        ],\n",
    "        \"check\": lambda: (\n",
    "            check_directory_has_files(paths[\"etl_base\"] / \"extracted\" / \"zepp\", \"*.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"apple_health_export\" / \"export.xml\")\n",
    "        ),\n",
    "        \"hint\": f\"make ingest PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 1: Aggregate\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_sleep.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_cardio.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_activity.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_sleep.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_cardio.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_activity.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: (\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_sleep.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_cardio.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_activity.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_sleep.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_cardio.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_activity.csv\")\n",
    "        ),\n",
    "        \"hint\": f\"make aggregate PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 2: Unify\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"joined\" / \"features_daily_unified.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"etl_base\"] / \"joined\" / \"features_daily_unified.csv\"),\n",
    "        \"hint\": f\"make unify PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 3: Label\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"joined\" / \"features_daily_labeled.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"etl_base\"] / \"joined\" / \"features_daily_labeled.csv\"),\n",
    "        \"hint\": f\"make label PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 4: Segment\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"segment_autolog.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"etl_base\"] / \"segment_autolog.csv\"),\n",
    "        \"hint\": f\"make segment PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 5: Prep NB2\",\n",
    "        \"key_files\": [\n",
    "            paths[\"ai_base\"] / \"nb2_dataset.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"ai_base\"] / \"nb2_dataset.csv\"),\n",
    "        \"hint\": f\"make prep-nb2 PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 6: NB2\",\n",
    "        \"key_files\": [\n",
    "            paths[\"ai_base\"] / \"nb2\",\n",
    "        ],\n",
    "        \"check\": lambda: check_directory_has_files(paths[\"ai_base\"] / \"nb2\", \"*.csv\"),\n",
    "        \"hint\": f\"make nb2 PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 7: NB3\",\n",
    "        \"key_files\": [\n",
    "            paths[\"ai_base\"] / \"nb3\",\n",
    "        ],\n",
    "        \"check\": lambda: check_directory_has_files(paths[\"ai_base\"] / \"nb3\", \"*.json\"),\n",
    "        \"hint\": f\"make nb3 PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"QC: Quality Control\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"qc\",\n",
    "        ],\n",
    "        \"check\": lambda: check_directory_has_files(paths[\"etl_base\"] / \"qc\", \"*.json\"),\n",
    "        \"hint\": f\"make qc-all PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"‚úì Stage detection logic configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b5b6d",
   "metadata": {},
   "source": [
    "## Stage Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c5c555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "PIPELINE STAGE STATUS\n",
      "====================================================================================================\n",
      "              Stage    Status                                                                                                                                                                                                                                                                                                                                                                         Key Files  Exists                                         Action\n",
      "    Stage 0: Ingest      ‚úÖ OK                                                                                                                                                                                                                                                                       data\\etl\\P000001\\2025-11-07\\extracted\\zepp, data\\etl\\P000001\\2025-11-07\\extracted\\apple\\apple_health_export    True                                               \n",
      " Stage 1: Aggregate ‚ùå Missing data\\etl\\P000001\\2025-11-07\\extracted\\apple\\daily_sleep.csv, data\\etl\\P000001\\2025-11-07\\extracted\\apple\\daily_cardio.csv, data\\etl\\P000001\\2025-11-07\\extracted\\apple\\daily_activity.csv, data\\etl\\P000001\\2025-11-07\\extracted\\zepp\\daily_sleep.csv, data\\etl\\P000001\\2025-11-07\\extracted\\zepp\\daily_cardio.csv, data\\etl\\P000001\\2025-11-07\\extracted\\zepp\\daily_activity.csv   False make aggregate PID=P000001 SNAPSHOT=2025-11-07\n",
      "     Stage 2: Unify ‚ùå Missing                                                                                                                                                                                                                                                                                                                     data\\etl\\P000001\\2025-11-07\\joined\\features_daily_unified.csv   False     make unify PID=P000001 SNAPSHOT=2025-11-07\n",
      "     Stage 3: Label ‚ùå Missing                                                                                                                                                                                                                                                                                                                     data\\etl\\P000001\\2025-11-07\\joined\\features_daily_labeled.csv   False     make label PID=P000001 SNAPSHOT=2025-11-07\n",
      "   Stage 4: Segment ‚ùå Missing                                                                                                                                                                                                                                                                                                                                   data\\etl\\P000001\\2025-11-07\\segment_autolog.csv   False   make segment PID=P000001 SNAPSHOT=2025-11-07\n",
      "  Stage 5: Prep NB2 ‚ùå Missing                                                                                                                                                                                                                                                                                                                                        data\\ai\\P000001\\2025-11-07\\nb2_dataset.csv   False  make prep-nb2 PID=P000001 SNAPSHOT=2025-11-07\n",
      "       Stage 6: NB2 ‚ùå Missing                                                                                                                                                                                                                                                                                                                                                    data\\ai\\P000001\\2025-11-07\\nb2   False       make nb2 PID=P000001 SNAPSHOT=2025-11-07\n",
      "       Stage 7: NB3 ‚ùå Missing                                                                                                                                                                                                                                                                                                                                                    data\\ai\\P000001\\2025-11-07\\nb3   False       make nb3 PID=P000001 SNAPSHOT=2025-11-07\n",
      "QC: Quality Control ‚ùå Missing                                                                                                                                                                                                                                                                                                                                                    data\\etl\\P000001\\2025-11-07\\qc   False    make qc-all PID=P000001 SNAPSHOT=2025-11-07\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run all checks\n",
    "results = []\n",
    "\n",
    "for check in stage_checks:\n",
    "    exists = check[\"check\"]()\n",
    "    status = \"‚úÖ OK\" if exists else \"‚ùå Missing\"\n",
    "    \n",
    "    results.append({\n",
    "        \"Stage\": check[\"stage\"],\n",
    "        \"Status\": status,\n",
    "        \"Key Files\": \", \".join([str(p.relative_to(REPO_ROOT)) for p in check[\"key_files\"]]),\n",
    "        \"Exists\": exists,\n",
    "        \"Action\": \"\" if exists else check[\"hint\"]\n",
    "    })\n",
    "\n",
    "# Create summary DataFrame\n",
    "df_status = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PIPELINE STAGE STATUS\")\n",
    "print(\"=\"*100)\n",
    "print(df_status.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74cdab",
   "metadata": {},
   "source": [
    "## Actionable Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0805ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  8 stage(s) incomplete:\n",
      "\n",
      "  ‚ùå Missing Stage 1: Aggregate\n",
      "     ‚Üí make aggregate PID=P000001 SNAPSHOT=2025-11-07\n",
      "\n",
      "  ‚ùå Missing Stage 2: Unify\n",
      "     ‚Üí make unify PID=P000001 SNAPSHOT=2025-11-07\n",
      "\n",
      "  ‚ùå Missing Stage 3: Label\n",
      "     ‚Üí make label PID=P000001 SNAPSHOT=2025-11-07\n",
      "\n",
      "  ‚ùå Missing Stage 4: Segment\n",
      "     ‚Üí make segment PID=P000001 SNAPSHOT=2025-11-07\n",
      "\n",
      "  ‚ùå Missing Stage 5: Prep NB2\n",
      "     ‚Üí make prep-nb2 PID=P000001 SNAPSHOT=2025-11-07\n",
      "\n",
      "  ‚ùå Missing Stage 6: NB2\n",
      "     ‚Üí make nb2 PID=P000001 SNAPSHOT=2025-11-07\n",
      "\n",
      "  ‚ùå Missing Stage 7: NB3\n",
      "     ‚Üí make nb3 PID=P000001 SNAPSHOT=2025-11-07\n",
      "\n",
      "  ‚ùå Missing QC: Quality Control\n",
      "     ‚Üí make qc-all PID=P000001 SNAPSHOT=2025-11-07\n",
      "\n",
      "\n",
      "üí° Recommendation: Run the first missing stage to proceed:\n",
      "   make aggregate PID=P000001 SNAPSHOT=2025-11-07\n"
     ]
    }
   ],
   "source": [
    "missing_stages = df_status[~df_status[\"Exists\"]]\n",
    "\n",
    "if len(missing_stages) == 0:\n",
    "    print(\"\\nüéâ All pipeline stages complete! Ready for NB1-NB3 analysis.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(missing_stages)} stage(s) incomplete:\\n\")\n",
    "    for idx, row in missing_stages.iterrows():\n",
    "        print(f\"  {row['Status']} {row['Stage']}\")\n",
    "        print(f\"     ‚Üí {row['Action']}\\n\")\n",
    "    \n",
    "    # First missing stage\n",
    "    first_missing = missing_stages.iloc[0]\n",
    "    print(\"\\nüí° Recommendation: Run the first missing stage to proceed:\")\n",
    "    print(f\"   {first_missing['Action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa36d48",
   "metadata": {},
   "source": [
    "## File Inventory (if stages complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad8b29ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è© Skipping inventory (Stage 2 not complete)\n"
     ]
    }
   ],
   "source": [
    "# Only run if at least Stage 2 is complete\n",
    "if check_file_exists(paths[\"etl_base\"] / \"joined\" / \"features_daily_unified.csv\"):\n",
    "    print(\"\\nüìä Quick Data Inventory:\\n\")\n",
    "    \n",
    "    # Load unified dataset\n",
    "    try:\n",
    "        df_unified = pd.read_csv(paths[\"etl_base\"] / \"joined\" / \"features_daily_unified.csv\")\n",
    "        print(f\"  features_daily_unified.csv:\")\n",
    "        print(f\"    - Shape: {df_unified.shape}\")\n",
    "        print(f\"    - Date range: {df_unified['date'].min()} to {df_unified['date'].max()}\")\n",
    "        print(f\"    - Columns: {', '.join(df_unified.columns[:10])}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Could not load features_daily_unified.csv: {e}\")\n",
    "    \n",
    "    # Load labeled dataset if available\n",
    "    if check_file_exists(paths[\"etl_base\"] / \"joined\" / \"features_daily_labeled.csv\"):\n",
    "        try:\n",
    "            df_labeled = pd.read_csv(paths[\"etl_base\"] / \"joined\" / \"features_daily_labeled.csv\")\n",
    "            print(f\"\\n  features_daily_labeled.csv:\")\n",
    "            print(f\"    - Shape: {df_labeled.shape}\")\n",
    "            if 'label_3cls' in df_labeled.columns:\n",
    "                print(f\"    - Label distribution: {df_labeled['label_3cls'].value_counts().to_dict()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Could not load features_daily_labeled.csv: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚è© Skipping inventory (Stage 2 not complete)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6058e9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a quick sanity check before running EDA or modelling notebooks.\n",
    "\n",
    "**Next steps**:\n",
    "- If all stages complete: proceed to **NB1_EDA.ipynb**\n",
    "- If missing stages: run the recommended commands above\n",
    "- For full pipeline run: `python -m scripts.run_full_pipeline --participant P000001 --snapshot 2025-11-07`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

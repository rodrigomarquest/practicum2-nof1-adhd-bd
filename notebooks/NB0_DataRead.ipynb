{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0ae5eb",
   "metadata": {},
   "source": [
    "# NB0: Data Readiness & Pipeline Stage Detection (v4.3.1)\n",
    "\n",
    "**Purpose**: Quickly verify if the necessary data for the N-of-1 pipeline are present and infer which ETL stages have been executed for P000001.\n",
    "\n",
    "**Pipeline**: practicum2-nof1-adhd-bd **v4.3.1** (Dec 10, 2025)  \n",
    "**Participant**: P000001  \n",
    "**Snapshot**: 2025-12-09\n",
    "\n",
    "This notebook is a sanity check / onboarding tool that:\n",
    "1. Detects presence of key directories and files\n",
    "2. Maps these to pipeline stages (0-9)\n",
    "3. Provides actionable hints for missing stages\n",
    "4. **v4.3.1**: Updated to detect MICE-imputed datasets in Stage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c28797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: c:\\dev\\practicum2-nof1-adhd-bd\n",
      "Checking data for: P000001 / 2025-12-09\n",
      "Data directory: c:\\dev\\practicum2-nof1-adhd-bd\\data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Configuration\n",
    "PARTICIPANT = \"P000001\"\n",
    "SNAPSHOT = \"2025-12-09\"\n",
    "\n",
    "# REPO_ROOT is one level up from notebooks/\n",
    "REPO_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Checking data for: {PARTICIPANT} / {SNAPSHOT}\")\n",
    "print(f\"Data directory: {REPO_ROOT / 'data'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5091b",
   "metadata": {},
   "source": [
    "## Stage Detection Logic\n",
    "\n",
    "Pipeline stages (from `scripts/run_full_pipeline.py`):\n",
    "\n",
    "- **Stage 0**: Ingest (extract from data/raw)\n",
    "- **Stage 1**: Aggregate (XML + CSVs to daily_*.csv)\n",
    "- **Stage 2**: Unify (merge daily CSVs)\n",
    "- **Stage 3**: Label (apply PBSI v4.3.1 labels - intuitive sign convention)\n",
    "- **Stage 4**: Segment (create segment_autolog.csv)\n",
    "- **Stage 5**: Prep ML6 (v4.3.1: temporal filter >= 2021-05-11 + MICE imputation)\n",
    "- **Stage 6**: ML6 (Static Classifier) (baseline models with 6-fold CV)\n",
    "- **Stage 7**: ML7 (LSTM Sequence) (LSTM + SHAP + drift detection)\n",
    "- **Stage 8**: TFLite (mobile deployment)\n",
    "- **Stage 9**: Report (RUN_REPORT.md)\n",
    "\n",
    "### v4.3.1 Changes (Dec 10, 2025)\n",
    "- **PBSI**: Higher score = better regulation (intuitive!)\n",
    "- **Stage 5**: Now outputs `ai/ml6/features_daily_ml6.csv` (MICE-imputed, 2021-2025)\n",
    "- **Stage 6**: Uses MICE data (F1=0.69¬±0.16)\n",
    "- **Stage 7**: SHAP + drift + LSTM on MICE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d043087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Stage detection logic configured (v4.3.1)\n"
     ]
    }
   ],
   "source": [
    "def check_file_exists(path: Path) -> bool:\n",
    "    \"\"\"Check if file exists and is non-empty.\"\"\"\n",
    "    return path.exists() and (path.is_dir() or path.stat().st_size > 0)\n",
    "\n",
    "def check_directory_has_files(path: Path, pattern: str = \"*\") -> bool:\n",
    "    \"\"\"Check if directory exists and contains files matching pattern.\"\"\"\n",
    "    if not path.exists() or not path.is_dir():\n",
    "        return False\n",
    "    return len(list(path.glob(pattern))) > 0\n",
    "\n",
    "# Define paths\n",
    "paths = {\n",
    "    \"raw_base\": REPO_ROOT / \"data\" / \"raw\",\n",
    "    \"etl_base\": REPO_ROOT / \"data\" / \"etl\" / PARTICIPANT / SNAPSHOT,\n",
    "    \"ai_base\": REPO_ROOT / \"data\" / \"ai\" / PARTICIPANT / SNAPSHOT,\n",
    "}\n",
    "\n",
    "# Stage-specific checks\n",
    "stage_checks = [\n",
    "    {\n",
    "        \"stage\": \"Stage 0: Ingest\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"zepp\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"apple_health_export\",\n",
    "        ],\n",
    "        \"check\": lambda: (\n",
    "            check_directory_has_files(paths[\"etl_base\"] / \"extracted\" / \"zepp\", \"*.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"apple_health_export\" / \"export.xml\")\n",
    "        ),\n",
    "        \"hint\": f\"make ingest PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 1: Aggregate\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_sleep.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_cardio.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_activity.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_sleep.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_cardio.csv\",\n",
    "            paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_activity.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: (\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_sleep.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_cardio.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"apple\" / \"daily_activity.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_sleep.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_cardio.csv\") or\n",
    "            check_file_exists(paths[\"etl_base\"] / \"extracted\" / \"zepp\" / \"daily_activity.csv\")\n",
    "        ),\n",
    "        \"hint\": f\"make aggregate PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 2: Unify\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"joined\" / \"features_daily_unified.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"etl_base\"] / \"joined\" / \"features_daily_unified.csv\"),\n",
    "        \"hint\": f\"make unify PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 3: Label (PBSI v4.3.1)\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"joined\" / \"features_daily_labeled.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"etl_base\"] / \"joined\" / \"features_daily_labeled.csv\"),\n",
    "        \"hint\": f\"make label PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 4: Segment\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"segment_autolog.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"etl_base\"] / \"segment_autolog.csv\"),\n",
    "        \"hint\": f\"make segment PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 5: Prep ML6 (v4.3.1: Temporal filter + MICE)\",\n",
    "        \"key_files\": [\n",
    "            paths[\"ai_base\"] / \"ml6\" / \"features_daily_ml6.csv\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"ai_base\"] / \"ml6\" / \"features_daily_ml6.csv\"),\n",
    "        \"hint\": f\"python scripts/run_full_pipeline.py --participant {PARTICIPANT} --snapshot {SNAPSHOT} --start-stage 5 --end-stage 5\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 6: ML6 (static classifier)\",\n",
    "        \"key_files\": [\n",
    "            paths[\"ai_base\"] / \"ml6\" / \"cv_summary.json\",\n",
    "        ],\n",
    "        \"check\": lambda: check_file_exists(paths[\"ai_base\"] / \"ml6\" / \"cv_summary.json\"),\n",
    "        \"hint\": f\"python scripts/run_full_pipeline.py --participant {PARTICIPANT} --snapshot {SNAPSHOT} --start-stage 6 --end-stage 6\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"Stage 7: ML7 (SHAP + Drift + LSTM)\",\n",
    "        \"key_files\": [\n",
    "            paths[\"ai_base\"] / \"ml7\" / \"shap_summary.md\",\n",
    "            paths[\"ai_base\"] / \"ml7\" / \"drift_report.md\",\n",
    "            paths[\"ai_base\"] / \"ml7\" / \"lstm_report.md\",\n",
    "        ],\n",
    "        \"check\": lambda: (\n",
    "            check_file_exists(paths[\"ai_base\"] / \"ml7\" / \"shap_summary.md\") and\n",
    "            check_file_exists(paths[\"ai_base\"] / \"ml7\" / \"drift_report.md\") and\n",
    "            check_file_exists(paths[\"ai_base\"] / \"ml7\" / \"lstm_report.md\")\n",
    "        ),\n",
    "        \"hint\": f\"python scripts/run_full_pipeline.py --participant {PARTICIPANT} --snapshot {SNAPSHOT} --start-stage 7 --end-stage 7\"\n",
    "    },\n",
    "    {\n",
    "        \"stage\": \"QC: Quality Control\",\n",
    "        \"key_files\": [\n",
    "            paths[\"etl_base\"] / \"qc\",\n",
    "        ],\n",
    "        \"check\": lambda: check_directory_has_files(paths[\"etl_base\"] / \"qc\", \"*.json\"),\n",
    "        \"hint\": f\"make qc-all PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"‚úì Stage detection logic configured (v4.3.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b5b6d",
   "metadata": {},
   "source": [
    "## Stage Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5c555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "PIPELINE STAGE STATUS\n",
      "====================================================================================================\n",
      "                                             Stage    Status                                                                                                                                                                                                                                                                                                                                                                         Key Files  Exists                                      Action\n",
      "                                   Stage 0: Ingest      ‚úÖ OK                                                                                                                                                                                                                                                                       data\\etl\\P000001\\2025-12-09\\extracted\\zepp, data\\etl\\P000001\\2025-12-09\\extracted\\apple\\apple_health_export    True                                            \n",
      "                                Stage 1: Aggregate      ‚úÖ OK data\\etl\\P000001\\2025-12-09\\extracted\\apple\\daily_sleep.csv, data\\etl\\P000001\\2025-12-09\\extracted\\apple\\daily_cardio.csv, data\\etl\\P000001\\2025-12-09\\extracted\\apple\\daily_activity.csv, data\\etl\\P000001\\2025-12-09\\extracted\\zepp\\daily_sleep.csv, data\\etl\\P000001\\2025-12-09\\extracted\\zepp\\daily_cardio.csv, data\\etl\\P000001\\2025-12-09\\extracted\\zepp\\daily_activity.csv    True                                            \n",
      "                                    Stage 2: Unify      ‚úÖ OK                                                                                                                                                                                                                                                                                                                     data\\etl\\P000001\\2025-12-09\\joined\\features_daily_unified.csv    True                                            \n",
      "                      Stage 3: Label (PBSI v4.3.1)      ‚úÖ OK                                                                                                                                                                                                                                                                                                                     data\\etl\\P000001\\2025-12-09\\joined\\features_daily_labeled.csv    True                                            \n",
      "                                  Stage 4: Segment      ‚úÖ OK                                                                                                                                                                                                                                                                                                                                   data\\etl\\P000001\\2025-12-09\\segment_autolog.csv    True                                            \n",
      "Stage 5: Prep ML6 (v4.3.1: Temporal filter + MICE)      ‚úÖ OK                                                                                                                                                                                                                                                                                                                             data\\ai\\P000001\\2025-12-09\\ml6\\features_daily_ml6.csv    True                                            \n",
      "                  Stage 6: ML6 (static classifier)      ‚úÖ OK                                                                                                                                                                                                                                                                                                                                    data\\ai\\P000001\\2025-12-09\\ml6\\cv_summary.json    True                                            \n",
      "                Stage 7: ML7 (SHAP + Drift + LSTM)      ‚úÖ OK                                                                                                                                                                                                                                     data\\ai\\P000001\\2025-12-09\\ml7\\shap_summary.md, data\\ai\\P000001\\2025-12-09\\ml7\\drift_report.md, data\\ai\\P000001\\2025-12-09\\ml7\\lstm_report.md    True                                            \n",
      "                               QC: Quality Control ‚ùå Missing                                                                                                                                                                                                                                                                                                                                                    data\\etl\\P000001\\2025-12-09\\qc   False make qc-all PID=P000001 SNAPSHOT=2025-12-09\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run all checks\n",
    "results = []\n",
    "\n",
    "for check in stage_checks:\n",
    "    exists = check[\"check\"]()\n",
    "    status = \"‚úÖ OK\" if exists else \"‚ùå Missing\"\n",
    "    \n",
    "    results.append({\n",
    "        \"Stage\": check[\"stage\"],\n",
    "        \"Status\": status,\n",
    "        \"Key Files\": \", \".join([str(p.relative_to(REPO_ROOT)) for p in check[\"key_files\"]]),\n",
    "        \"Exists\": exists,\n",
    "        \"Action\": \"\" if exists else check[\"hint\"]\n",
    "    })\n",
    "\n",
    "# Create summary DataFrame\n",
    "df_status = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PIPELINE STAGE STATUS\")\n",
    "print(\"=\"*100)\n",
    "print(df_status.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74cdab",
   "metadata": {},
   "source": [
    "## Actionable Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0805ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  1 stage(s) incomplete:\n",
      "\n",
      "  ‚ùå Missing QC: Quality Control\n",
      "     ‚Üí make qc-all PID=P000001 SNAPSHOT=2025-12-09\n",
      "\n",
      "\n",
      "üí° Recommendation: Run the first missing stage to proceed:\n",
      "   make qc-all PID=P000001 SNAPSHOT=2025-12-09\n"
     ]
    }
   ],
   "source": [
    "missing_stages = df_status[~df_status[\"Exists\"]]\n",
    "\n",
    "if len(missing_stages) == 0:\n",
    "    print(\"\\nüéâ All pipeline stages complete! Ready for NB1-NB3 analysis.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(missing_stages)} stage(s) incomplete:\\n\")\n",
    "    for idx, row in missing_stages.iterrows():\n",
    "        print(f\"  {row['Status']} {row['Stage']}\")\n",
    "        print(f\"     ‚Üí {row['Action']}\\n\")\n",
    "    \n",
    "    # First missing stage\n",
    "    first_missing = missing_stages.iloc[0]\n",
    "    print(\"\\nüí° Recommendation: Run the first missing stage to proceed:\")\n",
    "    print(f\"   {first_missing['Action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa36d48",
   "metadata": {},
   "source": [
    "## File Inventory (if stages complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8b29ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Quick Data Inventory:\n",
      "\n",
      "  features_daily_unified.csv:\n",
      "    - Shape: (2868, 30)\n",
      "    - Date range: 2017-12-04 to 2025-12-07\n",
      "    - Columns: date, sleep_hours, sleep_quality_score, hr_mean, hr_min, hr_max, hr_std, hr_samples, hrv_sdnn_mean, hrv_sdnn_median...\n",
      "\n",
      "  features_daily_labeled.csv:\n",
      "    - Shape: (2868, 53)\n",
      "    - Label distribution: {0.0: 1434, 1.0: 717, -1.0: 717}\n"
     ]
    }
   ],
   "source": [
    "# Only run if at least Stage 2 is complete\n",
    "if check_file_exists(paths[\"etl_base\"] / \"joined\" / \"features_daily_unified.csv\"):\n",
    "    print(\"\\nüìä Quick Data Inventory:\\n\")\n",
    "    \n",
    "    # Load unified dataset\n",
    "    try:\n",
    "        df_unified = pd.read_csv(paths[\"etl_base\"] / \"joined\" / \"features_daily_unified.csv\")\n",
    "        print(f\"  features_daily_unified.csv:\")\n",
    "        print(f\"    - Shape: {df_unified.shape}\")\n",
    "        print(f\"    - Date range: {df_unified['date'].min()} to {df_unified['date'].max()}\")\n",
    "        print(f\"    - Columns: {', '.join(df_unified.columns[:10])}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Could not load features_daily_unified.csv: {e}\")\n",
    "    \n",
    "    # Load labeled dataset if available\n",
    "    if check_file_exists(paths[\"etl_base\"] / \"joined\" / \"features_daily_labeled.csv\"):\n",
    "        try:\n",
    "            df_labeled = pd.read_csv(paths[\"etl_base\"] / \"joined\" / \"features_daily_labeled.csv\")\n",
    "            print(f\"\\n  features_daily_labeled.csv:\")\n",
    "            print(f\"    - Shape: {df_labeled.shape}\")\n",
    "            if 'label_3cls' in df_labeled.columns:\n",
    "                print(f\"    - Label distribution: {df_labeled['label_3cls'].value_counts().to_dict()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Could not load features_daily_labeled.csv: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚è© Skipping inventory (Stage 2 not complete)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6058e9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a quick sanity check before running EDA or modelling notebooks.\n",
    "\n",
    "**Next steps**:\n",
    "- If all stages complete: proceed to **NB1_EDA.ipynb**\n",
    "- If missing stages: run the recommended commands above\n",
    "- For full pipeline run: `python -m scripts.run_full_pipeline --participant P000001 --snapshot 2025-12-09`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

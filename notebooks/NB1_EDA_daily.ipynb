{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c0c13",
   "metadata": {},
   "source": [
    "# NB1: Daily ETL Validation & EDA\n",
    "\n",
    "**Purpose:** Validate daily ETL outputs from `joined_features_daily.csv` and surface actionable insights for researcher/participant.\n",
    "\n",
    "**Inputs:**\n",
    "- `data/etl/<PID>/<SNAPSHOT>/joined/joined_features_daily.csv` (primary source of truth)\n",
    "- Fallback: `features/<domain>/**/features_daily.csv` if primary unavailable\n",
    "- `enriched/prejoin/**/enriched_*.csv` (enriched features)\n",
    "\n",
    "**Outputs:**\n",
    "- `reports/nb1_eda_summary.md` ‚Äî Human-readable summary\n",
    "- `reports/nb1_feature_stats.csv` ‚Äî Per-column descriptive statistics\n",
    "- `reports/plots/*.png` ‚Äî Inline visualizations\n",
    "- `reports/nb1_manifest.json` ‚Äî Metadata & artifact manifest\n",
    "- `latest/` ‚Äî Mirror with symlinks for quick access\n",
    "\n",
    "**Version:** v1.0 | **Date:** 2025-11-06 | **Environment:** Local, offline, no internet calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fbb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import textwrap\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Log helper\n",
    "def log(msg):\n",
    "    print(f\"[NB1] {msg}\")\n",
    "\n",
    "log(\"Environment ready ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef351e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: CONFIG & PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# User Parameters\n",
    "PID = \"P000001\"\n",
    "SNAPSHOT = \"auto\"  # or \"YYYY-MM-DD\"\n",
    "\n",
    "# Base paths\n",
    "BASE = Path(\"data/etl\") / PID\n",
    "OUT = Path(\"reports\")\n",
    "PLOTS = OUT / \"plots\"\n",
    "LATEST = Path(\"latest\")\n",
    "\n",
    "# Resolve SNAPSHOT=\"auto\" to latest snapshot folder\n",
    "if SNAPSHOT == \"auto\":\n",
    "    if BASE.exists():\n",
    "        snapshots = sorted([d.name for d in BASE.iterdir() if d.is_dir()])\n",
    "        if snapshots:\n",
    "            SNAPSHOT = snapshots[-1]\n",
    "            log(f\"Resolved SNAPSHOT='auto' ‚Üí '{SNAPSHOT}'\")\n",
    "        else:\n",
    "            raise ValueError(f\"No snapshots found under {BASE}\")\n",
    "    else:\n",
    "        raise ValueError(f\"BASE path {BASE} does not exist\")\n",
    "else:\n",
    "    log(f\"Using SNAPSHOT='{SNAPSHOT}'\")\n",
    "\n",
    "# Define JOINED path\n",
    "JOINED = BASE / SNAPSHOT / \"joined\" / \"joined_features_daily.csv\"\n",
    "log(f\"JOINED path: {JOINED}\")\n",
    "\n",
    "# Create output directories\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS.mkdir(parents=True, exist_ok=True)\n",
    "LATEST.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify JOINED exists\n",
    "if not JOINED.exists():\n",
    "    raise FileNotFoundError(f\"JOINED file not found: {JOINED}\")\n",
    "\n",
    "log(f\"‚úì Config ready | PID={PID} | SNAPSHOT={SNAPSHOT} | OUT={OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fe5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: SAFE LOAD & BASIC SCHEMA\n",
    "# ============================================================================\n",
    "\n",
    "# Load with date parsing\n",
    "df = pd.read_csv(JOINED, parse_dates=['date'])\n",
    "\n",
    "# Normalize date to midnight\n",
    "df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Drop duplicate rows (keep latest by index)\n",
    "n_before = len(df)\n",
    "df = df.drop_duplicates(subset=['date'], keep='last').reset_index(drop=True)\n",
    "n_after = len(df)\n",
    "if n_before > n_after:\n",
    "    log(f\"‚ö† Removed {n_before - n_after} duplicate date rows\")\n",
    "\n",
    "# Basic info\n",
    "n_rows, n_cols = df.shape\n",
    "date_min = df['date'].min()\n",
    "date_max = df['date'].max()\n",
    "date_range = (date_max - date_min).days + 1\n",
    "\n",
    "log(f\"Loaded {n_rows} rows √ó {n_cols} cols | Date range: {date_min.date()} to {date_max.date()} ({date_range} days)\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nLast 5 rows:\")\n",
    "print(df.tail())\n",
    "\n",
    "# Verify monotonic dates\n",
    "if not df['date'].is_monotonic_increasing:\n",
    "    log(\"‚ö† Dates are not monotonic! Sorting...\")\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "else:\n",
    "    log(\"‚úì Dates are monotonic\")\n",
    "\n",
    "print(f\"\\nDataFrame Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c936a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: COLUMN INVENTORY & DTYPE COERCION\n",
    "# ============================================================================\n",
    "\n",
    "# Define expected columns by domain\n",
    "cols_activity_apple = ['apple_steps', 'apple_distance_m', 'apple_active_kcal', 'apple_exercise_min',\n",
    "                       'apple_stand_hours', 'apple_move_goal_kcal', 'apple_exercise_goal_min',\n",
    "                       'apple_stand_goal_hours', 'apple_rings_close_move', 'apple_rings_close_exercise',\n",
    "                       'apple_rings_close_stand']\n",
    "cols_activity_zepp = ['zepp_act_steps', 'zepp_act_distance_km', 'zepp_act_cal_active', 'zepp_act_cal_total',\n",
    "                      'zepp_act_sedentary_min', 'zepp_act_stand_hours', 'zepp_act_sport_sessions',\n",
    "                      'zepp_act_exercise_min', 'zepp_act_score_daily']\n",
    "cols_activity_coalesced = ['act_steps', 'act_active_min']\n",
    "cols_cardio = ['apple_hr_mean', 'apple_hr_std', 'apple_n_hr', 'zepp_hr_mean', 'zepp_hr_std',\n",
    "               'zepp_n_hr', 'hr_mean', 'hr_std', 'n_hr']\n",
    "cols_sleep = ['zepp_slp_total_h', 'zepp_slp_deep_h', 'zepp_slp_light_h', 'zepp_slp_rem_h', 'sleep_total_h']\n",
    "cols_label = ['label', 'label_source']\n",
    "cols_segment = ['segment_id']\n",
    "\n",
    "all_numeric_expected = (cols_activity_apple + cols_activity_zepp + cols_activity_coalesced +\n",
    "                        cols_cardio + cols_sleep)\n",
    "\n",
    "# Coerce numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['object', 'float64', 'int64']).columns\n",
    "for col in numeric_cols:\n",
    "    if col not in ['date', 'label', 'label_source', 'segment_id', 'source_domain']:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Report coverage by domain\n",
    "log(\"\\n=== COLUMN INVENTORY ===\")\n",
    "\n",
    "domains = {\n",
    "    'Activity (Apple)': cols_activity_apple,\n",
    "    'Activity (Zepp)': cols_activity_zepp,\n",
    "    'Activity (Coalesced)': cols_activity_coalesced,\n",
    "    'Cardio': cols_cardio,\n",
    "    'Sleep': cols_sleep,\n",
    "}\n",
    "\n",
    "for domain_name, cols in domains.items():\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    print(f\"\\n{domain_name}:\")\n",
    "    print(f\"  Present ({len(present)}): {present[:5]}{'...' if len(present) > 5 else ''}\")\n",
    "    if missing:\n",
    "        print(f\"  Missing ({len(missing)}): {missing[:5]}{'...' if len(missing) > 5 else ''}\")\n",
    "\n",
    "# Check for enriched columns (*_7d, *_zscore)\n",
    "enriched_7d = [c for c in df.columns if '_7d' in c]\n",
    "enriched_zscore = [c for c in df.columns if '_zscore' in c]\n",
    "print(f\"\\nEnriched (*_7d): {len(enriched_7d)} columns {enriched_7d[:3]}{'...' if len(enriched_7d) > 3 else ''}\")\n",
    "print(f\"Enriched (*_zscore): {len(enriched_zscore)} columns {enriched_zscore[:3]}{'...' if len(enriched_zscore) > 3 else ''}\")\n",
    "\n",
    "log(\"‚úì Column inventory complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: DATA HEALTH CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== DATA HEALTH CHECKS ===\")\n",
    "\n",
    "# 4.1 Missingness per column\n",
    "coverage = df.notna().mean() * 100\n",
    "coverage_sorted = coverage.sort_values()\n",
    "\n",
    "print(f\"\\nCoverage (% non-null) - Worst 15 columns:\")\n",
    "print(coverage_sorted.head(15))\n",
    "\n",
    "# Bar plot of worst 15\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "coverage_worst_15 = coverage_sorted.head(15)\n",
    "ax.barh(range(len(coverage_worst_15)), coverage_worst_15.values)\n",
    "ax.set_yticks(range(len(coverage_worst_15)))\n",
    "ax.set_yticklabels(coverage_worst_15.index)\n",
    "ax.set_xlabel('Coverage (%)')\n",
    "ax.set_title('Column Coverage: 15 Worst')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS / \"00_coverage_worst15.png\", dpi=144)\n",
    "plt.show()\n",
    "\n",
    "# 4.2 Date continuity\n",
    "expected_dates = pd.date_range(date_min, date_max, freq='D')\n",
    "actual_dates = set(df['date'].values)\n",
    "missing_dates = [d for d in expected_dates if d not in actual_dates]\n",
    "if missing_dates:\n",
    "    log(f\"‚ö† {len(missing_dates)} missing date(s)\")\n",
    "else:\n",
    "    log(\"‚úì No missing dates (continuous)\")\n",
    "\n",
    "# Plot daily non-null counts for core trio\n",
    "core_trio = ['act_steps', 'hr_mean', 'sleep_total_h']\n",
    "core_present = [c for c in core_trio if c in df.columns]\n",
    "\n",
    "if core_present:\n",
    "    daily_counts = df[[c for c in core_present]].notna().sum(axis=1)\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    ax.plot(df['date'], daily_counts, marker='o', markersize=2, linewidth=1)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Non-null count')\n",
    "    ax.set_title(f'Daily Data Availability: {core_present}')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS / \"00_date_continuity.png\", dpi=144)\n",
    "    plt.show()\n",
    "\n",
    "# 4.3 Duplicates\n",
    "n_unique_dates = df['date'].nunique()\n",
    "if n_unique_dates < len(df):\n",
    "    log(f\"‚ö† {len(df) - n_unique_dates} duplicate date row(s) found\")\n",
    "else:\n",
    "    log(\"‚úì All dates unique\")\n",
    "\n",
    "# 4.4 Value ranges\n",
    "ranges_checks = []\n",
    "\n",
    "if 'sleep_total_h' in df.columns:\n",
    "    bad_sleep = df[(df['sleep_total_h'] < 0) | (df['sleep_total_h'] > 16)]\n",
    "    if len(bad_sleep) > 0:\n",
    "        ranges_checks.append(f\"sleep_total_h out of [0, 16]: {len(bad_sleep)} rows\")\n",
    "\n",
    "if 'apple_distance_m' in df.columns:\n",
    "    bad_apple_dist = df[df['apple_distance_m'] > 50000]  # > 50km\n",
    "    if len(bad_apple_dist) > 0:\n",
    "        ranges_checks.append(f\"apple_distance_m > 50km: {len(bad_apple_dist)} rows\")\n",
    "\n",
    "if 'zepp_act_distance_km' in df.columns:\n",
    "    bad_zepp_dist = df[(df['zepp_act_distance_km'] < 0) | (df['zepp_act_distance_km'] > 100)]\n",
    "    if len(bad_zepp_dist) > 0:\n",
    "        ranges_checks.append(f\"zepp_act_distance_km out of [0, 100]: {len(bad_zepp_dist)} rows\")\n",
    "\n",
    "if ranges_checks:\n",
    "    log(\"‚ö† Value range issues found:\")\n",
    "    for check in ranges_checks:\n",
    "        log(f\"   {check}\")\n",
    "else:\n",
    "    log(\"‚úì Value ranges OK\")\n",
    "\n",
    "# 4.5 Rolling features sanity\n",
    "rolling_7d_cols = [c for c in df.columns if '_7d' in c]\n",
    "if rolling_7d_cols:\n",
    "    for col in rolling_7d_cols[:3]:  # Check first 3\n",
    "        nan_in_first_6 = df.iloc[:6][col].isna().sum()\n",
    "        if nan_in_first_6 < 6:\n",
    "            log(f\"‚ö† {col}: only {6 - nan_in_first_6} of first 6 values are NaN (expect 6)\")\n",
    "    log(\"‚úì Rolling features (*_7d) checked\")\n",
    "\n",
    "log(\"‚úì Data health checks complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc62f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: DESCRIPTIVE STATS PER DOMAIN\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "\n",
    "# Compute stats for all numeric columns\n",
    "stats_rows = []\n",
    "for col in df.columns:\n",
    "    if col in ['date', 'label', 'label_source', 'segment_id', 'source_domain']:\n",
    "        continue\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        continue\n",
    "    \n",
    "    non_null = df[col].notna().sum()\n",
    "    if non_null == 0:\n",
    "        continue\n",
    "    \n",
    "    stats_rows.append({\n",
    "        'column': col,\n",
    "        'non_null': non_null,\n",
    "        'mean': df[col].mean(),\n",
    "        'std': df[col].std(),\n",
    "        'min': df[col].min(),\n",
    "        'p25': df[col].quantile(0.25),\n",
    "        'p50': df[col].quantile(0.50),\n",
    "        'p75': df[col].quantile(0.75),\n",
    "        'max': df[col].max(),\n",
    "        'missing_pct': (1 - non_null / len(df)) * 100,\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats_rows).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "# Save to CSV\n",
    "stats_df.to_csv(OUT / \"nb1_feature_stats.csv\", index=False)\n",
    "log(f\"‚úì Saved nb1_feature_stats.csv ({len(stats_df)} columns)\")\n",
    "\n",
    "# Print top 20 with lowest coverage (most missing)\n",
    "print(\"\\nTop 20 columns with lowest coverage (worst coverage):\")\n",
    "print(stats_df.head(20)[['column', 'non_null', 'mean', 'std', 'missing_pct']])\n",
    "\n",
    "# Print top 20 with highest coverage (best coverage)\n",
    "print(\"\\nTop 20 columns with highest coverage (best coverage):\")\n",
    "print(stats_df.tail(20)[['column', 'non_null', 'mean', 'std', 'missing_pct']])\n",
    "\n",
    "log(\"‚úì Descriptive stats complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c232e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: QUICK INSIGHTS ‚Äì DAILY SIGNALS\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== QUICK INSIGHTS (Daily Signals) ===\")\n",
    "\n",
    "# 6.1 Activity: act_steps over time\n",
    "if 'act_steps' in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    ax.plot(df['date'], df['act_steps'], marker='o', markersize=3, linewidth=1, label='act_steps')\n",
    "    \n",
    "    # Annotate top 5 highs/lows\n",
    "    top5_high = df.nlargest(5, 'act_steps')\n",
    "    top5_low = df.nsmallest(5, 'act_steps')\n",
    "    \n",
    "    for idx, row in top5_high.iterrows():\n",
    "        if pd.notna(row['act_steps']):\n",
    "            ax.annotate(f\"{row['act_steps']:.0f}\", xy=(row['date'], row['act_steps']),\n",
    "                       xytext=(0, 5), textcoords='offset points', fontsize=7, ha='center')\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_title('Activity: Steps Over Time (Top 5 highs annotated)')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS / \"01_activity_steps.png\", dpi=144)\n",
    "    plt.show()\n",
    "\n",
    "# 6.2 Activity: exercise minutes if available\n",
    "if 'apple_exercise_min' in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    ax.plot(df['date'], df['apple_exercise_min'], marker='s', markersize=3, linewidth=1,\n",
    "            label='apple_exercise_min', color='orange')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Exercise Minutes')\n",
    "    ax.set_title('Activity: Apple Exercise Minutes')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS / \"01_activity_exercise_min.png\", dpi=144)\n",
    "    plt.show()\n",
    "\n",
    "# 6.3 Cardio: hr_mean and hr_std\n",
    "if 'hr_mean' in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    ax.plot(df['date'], df['hr_mean'], marker='o', markersize=3, linewidth=1, label='hr_mean', color='red')\n",
    "    \n",
    "    if 'hr_std' in df.columns:\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(df['date'], df['hr_std'], marker='s', markersize=3, linewidth=1, label='hr_std', color='blue')\n",
    "        ax2.set_ylabel('HR Std Dev', color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('HR Mean (bpm)', color='red')\n",
    "    ax.tick_params(axis='y', labelcolor='red')\n",
    "    ax.set_title('Cardio: HR Mean & Std Over Time')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(loc='upper left')\n",
    "    if 'hr_std' in df.columns:\n",
    "        ax2.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS / \"02_cardio_hr_mean_std.png\", dpi=144)\n",
    "    plt.show()\n",
    "\n",
    "# 6.4 Cardio: Apple vs Zepp HR if both present\n",
    "if 'apple_hr_mean' in df.columns and 'zepp_hr_mean' in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    ax.plot(df['date'], df['apple_hr_mean'], marker='o', markersize=2, linewidth=1,\n",
    "            label='apple_hr_mean', alpha=0.7)\n",
    "    ax.plot(df['date'], df['zepp_hr_mean'], marker='s', markersize=2, linewidth=1,\n",
    "            label='zepp_hr_mean', alpha=0.7)\n",
    "    if 'hr_mean' in df.columns:\n",
    "        ax.plot(df['date'], df['hr_mean'], marker='^', markersize=2, linewidth=1,\n",
    "                label='hr_mean (coalesced)', alpha=0.9)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('HR Mean (bpm)')\n",
    "    ax.set_title('Cardio: Apple vs Zepp vs Coalesced HR Mean')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS / \"02_cardio_apple_vs_zepp.png\", dpi=144)\n",
    "    plt.show()\n",
    "\n",
    "# 6.5 Sleep: sleep_total_h with components\n",
    "if 'sleep_total_h' in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    ax.plot(df['date'], df['sleep_total_h'], marker='o', markersize=3, linewidth=1.5,\n",
    "            label='sleep_total_h', color='purple')\n",
    "    \n",
    "    if 'zepp_slp_deep_h' in df.columns:\n",
    "        ax.plot(df['date'], df['zepp_slp_deep_h'], marker='s', markersize=2, linewidth=1,\n",
    "                label='zepp_slp_deep_h', alpha=0.7)\n",
    "    if 'zepp_slp_light_h' in df.columns:\n",
    "        ax.plot(df['date'], df['zepp_slp_light_h'], marker='^', markersize=2, linewidth=1,\n",
    "                label='zepp_slp_light_h', alpha=0.7)\n",
    "    if 'zepp_slp_rem_h' in df.columns:\n",
    "        ax.plot(df['date'], df['zepp_slp_rem_h'], marker='d', markersize=2, linewidth=1,\n",
    "                label='zepp_slp_rem_h', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Sleep (hours)')\n",
    "    ax.set_title('Sleep: Total Hours & Components')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS / \"03_sleep_total_components.png\", dpi=144)\n",
    "    plt.show()\n",
    "\n",
    "log(\"‚úì Daily signals plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: SEGMENT-AWARE VIEWS (S1‚ÄìS6)\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== SEGMENT-AWARE VIEWS ===\")\n",
    "\n",
    "if 'segment_id' in df.columns:\n",
    "    segments = sorted(df['segment_id'].dropna().unique())\n",
    "    log(f\"Found {len(segments)} segments: {segments}\")\n",
    "    \n",
    "    # Key features per segment\n",
    "    key_features = ['act_steps', 'hr_mean', 'sleep_total_h']\n",
    "    key_present = [c for c in key_features if c in df.columns]\n",
    "    \n",
    "    if key_present:\n",
    "        # Boxplots\n",
    "        fig, axes = plt.subplots(1, len(key_present), figsize=(15, 4))\n",
    "        if len(key_present) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax_idx, feature in enumerate(key_present):\n",
    "            data_by_seg = [df[df['segment_id'] == seg][feature].dropna().values for seg in segments]\n",
    "            ax = axes[ax_idx]\n",
    "            bp = ax.boxplot(data_by_seg, labels=segments)\n",
    "            ax.set_xlabel('Segment')\n",
    "            ax.set_ylabel(feature)\n",
    "            ax.set_title(f'{feature} by Segment')\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PLOTS / \"04_segments_boxplot.png\", dpi=144)\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary table: mean/std per segment\n",
    "        seg_stats = []\n",
    "        for seg in segments:\n",
    "            seg_df = df[df['segment_id'] == seg]\n",
    "            for feature in key_present:\n",
    "                seg_stats.append({\n",
    "                    'segment': seg,\n",
    "                    'feature': feature,\n",
    "                    'mean': seg_df[feature].mean(),\n",
    "                    'std': seg_df[feature].std(),\n",
    "                    'n': seg_df[feature].notna().sum(),\n",
    "                })\n",
    "        \n",
    "        seg_stats_df = pd.DataFrame(seg_stats)\n",
    "        print(\"\\nSegment Summary Statistics:\")\n",
    "        print(seg_stats_df.to_string(index=False))\n",
    "        \n",
    "        # Insights\n",
    "        print(\"\\n--- Segment Insights ---\")\n",
    "        for feature in key_present:\n",
    "            feat_stats = seg_stats_df[seg_stats_df['feature'] == feature].sort_values('mean', ascending=False)\n",
    "            best_seg = feat_stats.iloc[0]\n",
    "            worst_seg = feat_stats.iloc[-1]\n",
    "            print(f\"{feature}:\")\n",
    "            print(f\"  Best: {best_seg['segment']} (Œº={best_seg['mean']:.1f}, œÉ={best_seg['std']:.1f})\")\n",
    "            print(f\"  Worst: {worst_seg['segment']} (Œº={worst_seg['mean']:.1f}, œÉ={worst_seg['std']:.1f})\")\n",
    "else:\n",
    "    log(\"‚ìò No segment_id column; skipping segment-aware views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: LABELS COVERAGE (If Present)\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== LABELS COVERAGE ===\")\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    label_coverage = df['label'].notna().sum() / len(df) * 100\n",
    "    log(f\"Label coverage: {label_coverage:.1f}%\")\n",
    "    \n",
    "    # Contingency by class\n",
    "    label_counts = df['label'].value_counts()\n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    print(label_counts)\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    if len(label_counts) > 0:\n",
    "        max_class_pct = label_counts.iloc[0] / label_counts.sum() * 100\n",
    "        if max_class_pct > 80:\n",
    "            log(f\"‚ö† Class imbalance detected: majority class {label_counts.index[0]} is {max_class_pct:.1f}%\")\n",
    "        else:\n",
    "            log(f\"‚úì Class balance OK (majority class: {max_class_pct:.1f}%)\")\n",
    "    \n",
    "    # Bar plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    label_counts.plot(kind='bar', ax=ax, color='steelblue')\n",
    "    ax.set_xlabel('Label')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Label Distribution')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS / \"05_label_distribution.png\", dpi=144)\n",
    "    plt.show()\n",
    "    \n",
    "    # Coverage by segment if available\n",
    "    if 'segment_id' in df.columns:\n",
    "        print(\"\\nLabel Coverage by Segment:\")\n",
    "        seg_label_cov = df.groupby('segment_id')['label'].apply(lambda x: x.notna().sum() / len(x) * 100)\n",
    "        print(seg_label_cov)\n",
    "else:\n",
    "    log(\"‚ìò No label column; skipping label coverage analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0450a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 9: CORRELATIONS & CROSS-DOMAIN HINTS\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== CORRELATIONS & CROSS-DOMAIN HINTS ===\")\n",
    "\n",
    "# Curated subset for correlation\n",
    "corr_cols = []\n",
    "for col in ['act_steps', 'act_active_min', 'hr_mean', 'hr_std', 'sleep_total_h']:\n",
    "    if col in df.columns:\n",
    "        corr_cols.append(col)\n",
    "\n",
    "# Add enriched *_7d columns\n",
    "enriched_7d = [c for c in df.columns if '_7d' in c and pd.api.types.is_numeric_dtype(df[c])]\n",
    "corr_cols.extend(enriched_7d[:5])  # First 5 enriched\n",
    "\n",
    "corr_cols = list(set(corr_cols))  # Remove duplicates\n",
    "\n",
    "if len(corr_cols) > 1:\n",
    "    # Compute Spearman correlation\n",
    "    corr_matrix = df[corr_cols].corr(method='spearman')\n",
    "    \n",
    "    # Flatten to show all correlations sorted by absolute value\n",
    "    corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            col1 = corr_matrix.columns[i]\n",
    "            col2 = corr_matrix.columns[j]\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            corr_pairs.append({\n",
    "                'var1': col1,\n",
    "                'var2': col2,\n",
    "                'spearman_rho': corr_val,\n",
    "                'abs_rho': abs(corr_val),\n",
    "            })\n",
    "    \n",
    "    corr_pairs_df = pd.DataFrame(corr_pairs).sort_values('abs_rho', ascending=False)\n",
    "    print(\"\\nSpearman Correlations (top 15):\")\n",
    "    print(corr_pairs_df.head(15)[['var1', 'var2', 'spearman_rho']])\n",
    "    \n",
    "    # Scatter plots for key relationships\n",
    "    scatter_pairs = [\n",
    "        ('act_steps', 'sleep_total_h'),\n",
    "        ('hr_mean', 'sleep_total_h'),\n",
    "        ('act_active_min', 'hr_mean'),\n",
    "    ]\n",
    "    \n",
    "    for var1, var2 in scatter_pairs:\n",
    "        if var1 in df.columns and var2 in df.columns:\n",
    "            data = df[[var1, var2]].dropna()\n",
    "            \n",
    "            if len(data) > 300:\n",
    "                data = data.sample(300, random_state=42)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(8, 5))\n",
    "            ax.scatter(data[var1], data[var2], alpha=0.5, s=30)\n",
    "            ax.set_xlabel(var1)\n",
    "            ax.set_ylabel(var2)\n",
    "            ax.set_title(f'Scatter: {var1} vs {var2} (n={len(data)})')\n",
    "            ax.grid(alpha=0.3)\n",
    "            \n",
    "            # Add correlation info\n",
    "            rho, pval = spearmanr(data[var1], data[var2])\n",
    "            ax.text(0.05, 0.95, f\"œÅ={rho:.2f} (p={pval:.2e})\", transform=ax.transAxes,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PLOTS / f\"06_scatter_{var1}_vs_{var2}.png\", dpi=144)\n",
    "            plt.show()\n",
    "else:\n",
    "    log(\"‚ö† Insufficient columns for correlation analysis\")\n",
    "\n",
    "log(\"‚úì Correlations computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa400197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 10: BEST/WORST DAYS & CANDIDATE PATTERNS\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== BEST/WORST DAYS & CANDIDATE PATTERNS ===\")\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Top 10 best sleep days\n",
    "if 'sleep_total_h' in df.columns:\n",
    "    best_sleep = df.nlargest(10, 'sleep_total_h')[['date', 'sleep_total_h', 'act_steps', 'hr_mean']]\n",
    "    print(\"\\n--- Top 10 BEST SLEEP Days ---\")\n",
    "    print(best_sleep.to_string(index=False))\n",
    "    insights.append(\"High sleep nights: check if preceded by high activity or low exercise.\")\n",
    "\n",
    "# Top 10 high activity days\n",
    "if 'act_steps' in df.columns:\n",
    "    high_activity = df.nlargest(10, 'act_steps')[['date', 'act_steps', 'sleep_total_h', 'hr_mean']]\n",
    "    print(\"\\n--- Top 10 HIGH ACTIVITY Days ---\")\n",
    "    print(high_activity.to_string(index=False))\n",
    "    insights.append(\"High activity days: check correlation with next-day sleep quality.\")\n",
    "\n",
    "# Exploratory hypotheses\n",
    "print(\"\\n--- EXPLORATORY HYPOTHESES (Candidate Patterns) ---\")\n",
    "print(\"‚Ä¢ High activity days tend to precede higher sleep quality on next day?\")\n",
    "print(\"‚Ä¢ Days with extreme HR spikes correlate with low sleep?\")\n",
    "print(\"‚Ä¢ Activity patterns stable within segments; sleep varies more?\")\n",
    "print(\"‚Ä¢ Enriched *_7d features capture rolling trends better than daily noise.\")\n",
    "\n",
    "log(\"‚úì Best/worst days identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01433c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 11: MANIFEST & SUMMARY GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== GENERATING MANIFEST & SUMMARY ===\")\n",
    "\n",
    "# Build manifest\n",
    "manifest = {\n",
    "    'pid': PID,\n",
    "    'snapshot': SNAPSHOT,\n",
    "    'input_path': str(JOINED),\n",
    "    'row_count': int(n_rows),\n",
    "    'col_count': int(n_cols),\n",
    "    'date_range': {\n",
    "        'min': str(date_min.date()),\n",
    "        'max': str(date_max.date()),\n",
    "        'days': int(date_range),\n",
    "    },\n",
    "    'domain_coverage': {\n",
    "        'activity_apple': sum(1 for c in cols_activity_apple if c in df.columns),\n",
    "        'activity_zepp': sum(1 for c in cols_activity_zepp if c in df.columns),\n",
    "        'activity_coalesced': sum(1 for c in cols_activity_coalesced if c in df.columns),\n",
    "        'cardio': sum(1 for c in cols_cardio if c in df.columns),\n",
    "        'sleep': sum(1 for c in cols_sleep if c in df.columns),\n",
    "        'enriched_7d': len(enriched_7d),\n",
    "        'enriched_zscore': len(enriched_zscore),\n",
    "    },\n",
    "    'missing_per_domain': {\n",
    "        'activity_apple': len([c for c in cols_activity_apple if c not in df.columns]),\n",
    "        'activity_zepp': len([c for c in cols_activity_zepp if c not in df.columns]),\n",
    "        'cardio': len([c for c in cols_cardio if c not in df.columns]),\n",
    "        'sleep': len([c for c in cols_sleep if c not in df.columns]),\n",
    "    },\n",
    "    'plots_saved': sorted([p.name for p in PLOTS.glob('*.png')]),\n",
    "    'generation_timestamp': datetime.now().isoformat(),\n",
    "    'notebook_version': '1.0',\n",
    "}\n",
    "\n",
    "# Save manifest\n",
    "manifest_path = OUT / \"nb1_manifest.json\"\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "log(f\"‚úì Saved manifest: {manifest_path}\")\n",
    "\n",
    "# Generate summary markdown\n",
    "summary_lines = [\n",
    "    \"# NB1 EDA Daily Summary\",\n",
    "    f\"\\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    f\"**Participant:** {PID} | **Snapshot:** {SNAPSHOT}\",\n",
    "    f\"\\n## Coverage Overview\",\n",
    "    f\"\\n- **Rows:** {n_rows} | **Columns:** {n_cols} | **Date Range:** {date_range} days\",\n",
    "    f\"- **Activity (Apple):** {manifest['domain_coverage']['activity_apple']}/{len(cols_activity_apple)} columns\",\n",
    "    f\"- **Activity (Zepp):** {manifest['domain_coverage']['activity_zepp']}/{len(cols_activity_zepp)} columns\",\n",
    "    f\"- **Cardio:** {manifest['domain_coverage']['cardio']}/{len(cols_cardio)} columns\",\n",
    "    f\"- **Sleep:** {manifest['domain_coverage']['sleep']}/{len(cols_sleep)} columns\",\n",
    "    f\"- **Enriched (*_7d):** {len(enriched_7d)} columns\",\n",
    "    f\"\\n## Data Quality Observations\",\n",
    "]\n",
    "\n",
    "# Add data quality notes\n",
    "missing_counts = stats_df.nlargest(5, 'missing_pct')\n",
    "summary_lines.append(f\"\\n**Top 5 Most Missing Columns:**\\n\")\n",
    "for _, row in missing_counts.iterrows():\n",
    "    summary_lines.append(f\"- {row['column']}: {row['missing_pct']:.1f}% missing\")\n",
    "\n",
    "summary_lines.extend([\n",
    "    f\"\\n**Date Continuity:** {'‚úì Continuous' if len(missing_dates) == 0 else f'‚ö† {len(missing_dates)} gaps'}\",\n",
    "    f\"**Duplicate Dates:** {'‚úì None' if n_unique_dates == n_rows else f'‚ö† {n_rows - n_unique_dates} found'}\",\n",
    "    f\"\\n## Key Insights & Hypotheses\",\n",
    "    f\"\\n1. **Activity Patterns:** \",\n",
    "    f\"   - {df['act_steps'].mean():.0f} mean steps/day (œÉ={df['act_steps'].std():.0f})\" if 'act_steps' in df.columns else \"   - No activity data\",\n",
    "    f\"\\n2. **Cardiac Health:** \",\n",
    "    f\"   - {df['hr_mean'].mean():.1f} mean HR bpm (œÉ={df['hr_mean'].std():.1f})\" if 'hr_mean' in df.columns else \"   - No HR data\",\n",
    "    f\"\\n3. **Sleep Quality:** \",\n",
    "    f\"   - {df['sleep_total_h'].mean():.1f} mean sleep hours (œÉ={df['sleep_total_h'].std():.1f})\" if 'sleep_total_h' in df.columns else \"   - No sleep data\",\n",
    "    f\"\\n4. **Cross-Domain Signals:** High activity days may precede better sleep (exploratory).\",\n",
    "    f\"\\n5. **Enriched Features:** {len(enriched_7d)} rolling (*_7d) features available for smoothing/trend detection.\",\n",
    "    f\"\\n## Artifacts Generated\",\n",
    "    f\"\\n- `nb1_feature_stats.csv` ‚Äî Descriptive statistics (mean, std, quantiles, coverage %)\",\n",
    "    f\"- `plots/` ‚Äî {len(manifest['plots_saved'])} PNG visualizations\",\n",
    "    f\"- `nb1_manifest.json` ‚Äî Metadata & artifact manifest\",\n",
    "    f\"- `latest/` ‚Äî Mirrored copies for quick access\",\n",
    "    f\"\\n## Next Steps\",\n",
    "    f\"\\n1. **Data Cleaning:** Address worst-coverage columns (see feature_stats.csv).\",\n",
    "    f\"2. **Monitoring:** Watch for sudden value changes or gaps in time series.\",\n",
    "    f\"3. **Segmentation:** Analyze best/worst segments for domain-specific patterns.\",\n",
    "    f\"4. **Enrichment:** Leverage *_7d rolling features for signal smoothing.\",\n",
    "]\n",
    "\n",
    "summary_md = \"\\n\".join(summary_lines)\n",
    "\n",
    "# Save summary\n",
    "summary_path = OUT / \"nb1_eda_summary.md\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(summary_md)\n",
    "log(f\"‚úì Saved summary: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(summary_md)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 12: LATEST/ MIRROR SETUP\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n=== SETTING UP LATEST/ MIRROR ===\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Copy key artifacts to latest/\n",
    "key_artifacts = [\n",
    "    (OUT / \"nb1_eda_summary.md\", \"nb1_eda_summary.md\"),\n",
    "    (OUT / \"nb1_feature_stats.csv\", \"nb1_feature_stats.csv\"),\n",
    "    (OUT / \"nb1_manifest.json\", \"nb1_manifest.json\"),\n",
    "]\n",
    "\n",
    "for src, name in key_artifacts:\n",
    "    if src.exists():\n",
    "        dst = LATEST / name\n",
    "        shutil.copy2(src, dst)\n",
    "        log(f\"  Copied {name} ‚Üí latest/\")\n",
    "\n",
    "# Copy a few key plots\n",
    "key_plots = [\n",
    "    \"00_coverage_worst15.png\",\n",
    "    \"00_date_continuity.png\",\n",
    "    \"01_activity_steps.png\",\n",
    "    \"02_cardio_hr_mean_std.png\",\n",
    "    \"03_sleep_total_components.png\",\n",
    "]\n",
    "\n",
    "for plot_name in key_plots:\n",
    "    src = PLOTS / plot_name\n",
    "    if src.exists():\n",
    "        dst = LATEST / plot_name\n",
    "        shutil.copy2(src, dst)\n",
    "        log(f\"  Copied {plot_name} ‚Üí latest/\")\n",
    "\n",
    "log(\"‚úì Latest/ mirror ready\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ NB1 EDA COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÅ Artifacts Location: {OUT}\")\n",
    "print(f\"\\nüìä Key Files:\")\n",
    "print(f\"   - {OUT / 'nb1_eda_summary.md'}\")\n",
    "print(f\"   - {OUT / 'nb1_feature_stats.csv'}\")\n",
    "print(f\"   - {OUT / 'nb1_manifest.json'}\")\n",
    "print(f\"   - {PLOTS} (contains {len(list(PLOTS.glob('*.png')))} PNG files)\")\n",
    "print(f\"\\nüîó Quick Access: {LATEST}\")\n",
    "print(f\"\\nüìà Generated {len(list(PLOTS.glob('*.png')))} visualizations\")\n",
    "print(f\"‚ú® All data local, no internet calls. Ready for offline analysis.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

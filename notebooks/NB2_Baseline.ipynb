{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920b1eb1",
   "metadata": {},
   "source": [
    "# NB2: Baseline Models (Logistic Regression) ‚Äî Stage 6 (ML6)\n",
    "\n",
    "**Purpose**: Demonstrate Stage 6 (ML6) baseline behaviour and results using deterministic logistic regression with calendar-based cross-validation.\n",
    "\n",
    "**Pipeline**: practicum2-nof1-adhd-bd v4.1.x  \n",
    "**Participant**: P000001  \n",
    "**Snapshot**: 2025-11-07\n",
    "\n",
    "**Note**: This notebook uses the filename `NB2` for historical continuity, but internally refers to **Stage 6 (ML6)** following the refactoring to distinguish modeling stages from Jupyter notebook numbering.\n",
    "\n",
    "This notebook:\n",
    "1. Loads ML6 outputs (per-fold metrics, confusion matrices)\n",
    "2. Visualizes performance across folds\n",
    "3. Compares ML6 vs baselines (dummy, naive, rule-based)\n",
    "4. Shows predictions vs ground truth over time\n",
    "5. Provides markdown commentary on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4184e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "PARTICIPANT = \"P000001\"\n",
    "SNAPSHOT = \"2025-11-07\"\n",
    "REPO_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "\n",
    "AI_BASE = REPO_ROOT / \"data\" / \"ai\" / PARTICIPANT / SNAPSHOT\n",
    "ML6_DIR = AI_BASE / \"ml6\"  # Stage 6: Static daily classifier (formerly nb2)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"ML6 outputs (Stage 6): {ML6_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3954c1",
   "metadata": {},
   "source": [
    "## 1. Load ML6 Results (Stage 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ML6_DIR.exists():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚ùå ML6 OUTPUTS NOT FOUND (Stage 6: Static Classifier)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nRequired directory missing: {ML6_DIR}\")\n",
    "    print(\"\\nüìã To generate ML6 baseline model results, run:\")\n",
    "    print(f\"\\n   make ml6 PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\")\n",
    "    print(\"\\nüìù Note: This requires completed ETL stages 0-5\")\n",
    "    print(\"   If you haven't run the pipeline yet, use:\")\n",
    "    print(f\"   make pipeline PID={PARTICIPANT} SNAPSHOT={SNAPSHOT}\")\n",
    "    print(\"\\nüí° Check NB0_DataRead.ipynb to see which stages are complete\")\n",
    "    print(\"=\" * 80)\n",
    "    raise FileNotFoundError(f\"ML6 outputs not ready. See instructions above.\")\n",
    "\n",
    "# List available files\n",
    "ml6_files = list(ML6_DIR.glob(\"*\"))\n",
    "print(f\"\\nFound {len(ml6_files)} files in ML6 directory:\")\n",
    "for f in sorted(ml6_files)[:10]:\n",
    "    print(f\"  {f.name}\")\n",
    "\n",
    "# Load metrics CSV if available\n",
    "metrics_file = ML6_DIR / \"metrics_summary.csv\"\n",
    "if metrics_file.exists():\n",
    "    df_metrics = pd.read_csv(metrics_file)\n",
    "    print(f\"\\n‚úì Loaded metrics_summary.csv: {df_metrics.shape}\")\n",
    "    print(df_metrics.head())\n",
    "else:\n",
    "    # Try to find per-fold metrics\n",
    "    fold_files = list(ML6_DIR.glob(\"fold_*_metrics.csv\"))\n",
    "    if fold_files:\n",
    "        df_list = []\n",
    "        for f in sorted(fold_files):\n",
    "            df_fold = pd.read_csv(f)\n",
    "            fold_num = int(f.stem.split(\"_\")[1])\n",
    "            df_fold['fold'] = fold_num\n",
    "            df_list.append(df_fold)\n",
    "        df_metrics = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"\\n‚úì Loaded {len(fold_files)} per-fold metrics files\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No metrics files found. Checking for JSON...\")\n",
    "        df_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4eeb5b",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cac41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_metrics is not None and not df_metrics.empty:\n",
    "    # Key metrics to plot\n",
    "    metric_cols = ['macro_f1', 'weighted_f1', 'balanced_accuracy', 'auroc_ovr', 'cohens_kappa']\n",
    "    available_metrics = [m for m in metric_cols if m in df_metrics.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        fig, axes = plt.subplots(1, len(available_metrics), figsize=(16, 5))\n",
    "        if len(available_metrics) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, metric in enumerate(available_metrics):\n",
    "            if 'fold' in df_metrics.columns:\n",
    "                # Per-fold bar chart\n",
    "                fold_values = df_metrics.groupby('fold')[metric].mean()\n",
    "                axes[idx].bar(fold_values.index, fold_values.values, alpha=0.7, color='steelblue')\n",
    "                axes[idx].axhline(fold_values.mean(), color='red', linestyle='--', \n",
    "                                  linewidth=2, label=f'Mean: {fold_values.mean():.3f}')\n",
    "                axes[idx].set_xlabel('Fold', fontweight='bold')\n",
    "            else:\n",
    "                # Overall bar\n",
    "                axes[idx].bar([metric], [df_metrics[metric].mean()], alpha=0.7, color='steelblue')\n",
    "            \n",
    "            axes[idx].set_ylabel(metric.replace('_', ' ').title(), fontweight='bold')\n",
    "            axes[idx].set_title(f'{metric.upper()}', fontweight='bold')\n",
    "            axes[idx].set_ylim([0, 1])\n",
    "            axes[idx].legend()\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n‚úì Performance metrics visualization complete\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No standard metrics found in CSV\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Metrics DataFrame not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2bfa3b",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for confusion matrix files\n",
    "cm_files = list(NB2_DIR.glob(\"*confusion*\")) + list(NB2_DIR.glob(\"*cm*\"))\n",
    "\n",
    "if cm_files:\n",
    "    print(f\"Found {len(cm_files)} confusion matrix files\")\n",
    "    \n",
    "    # Try to load the first one\n",
    "    cm_file = cm_files[0]\n",
    "    print(f\"Loading: {cm_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        if cm_file.suffix == '.npy':\n",
    "            cm = np.load(cm_file)\n",
    "        elif cm_file.suffix == '.csv':\n",
    "            cm = pd.read_csv(cm_file, index_col=0).values\n",
    "        else:\n",
    "            cm = None\n",
    "        \n",
    "        if cm is not None:\n",
    "            # Normalize\n",
    "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "            \n",
    "            # Raw counts\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "                        xticklabels=['Stable', 'Neutral', 'Unstable'],\n",
    "                        yticklabels=['Stable', 'Neutral', 'Unstable'])\n",
    "            axes[0].set_xlabel('Predicted', fontweight='bold')\n",
    "            axes[0].set_ylabel('True', fontweight='bold')\n",
    "            axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
    "            \n",
    "            # Normalized\n",
    "            sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens', ax=axes[1],\n",
    "                        xticklabels=['Stable', 'Neutral', 'Unstable'],\n",
    "                        yticklabels=['Stable', 'Neutral', 'Unstable'])\n",
    "            axes[1].set_xlabel('Predicted', fontweight='bold')\n",
    "            axes[1].set_ylabel('True', fontweight='bold')\n",
    "            axes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\n‚úì Confusion matrix visualization complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Error loading confusion matrix: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No confusion matrix files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd5d99",
   "metadata": {},
   "source": [
    "## 4. Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for baseline comparison files\n",
    "baseline_files = list(NB2_DIR.glob(\"*baseline*\")) + list(NB2_DIR.glob(\"*comparison*\"))\n",
    "\n",
    "if baseline_files:\n",
    "    print(f\"Found {len(baseline_files)} baseline comparison files\")\n",
    "    \n",
    "    baseline_file = baseline_files[0]\n",
    "    try:\n",
    "        if baseline_file.suffix == '.csv':\n",
    "            df_baselines = pd.read_csv(baseline_file)\n",
    "            \n",
    "            if 'model' in df_baselines.columns and 'macro_f1' in df_baselines.columns:\n",
    "                # Plot comparison\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                \n",
    "                models = df_baselines['model'].values\n",
    "                scores = df_baselines['macro_f1'].values\n",
    "                \n",
    "                colors = ['red' if 'baseline' in m.lower() or 'dummy' in m.lower() \n",
    "                          else 'green' for m in models]\n",
    "                \n",
    "                ax.barh(models, scores, color=colors, alpha=0.7)\n",
    "                ax.set_xlabel('Macro F1 Score', fontweight='bold')\n",
    "                ax.set_title('NB2 vs Baselines Comparison', fontweight='bold', fontsize=14)\n",
    "                ax.grid(True, alpha=0.3, axis='x')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"\\n‚úì Baseline comparison visualization complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Error loading baseline comparison: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No baseline comparison files found\")\n",
    "    print(\"\\nüí° Typical baselines to consider:\")\n",
    "    print(\"   - Dummy (most frequent): F1 ‚âà 0.15-0.25\")\n",
    "    print(\"   - Naive (previous day): F1 ‚âà 0.30-0.40\")\n",
    "    print(\"   - Rule-based (simple thresholds): F1 ‚âà 0.40-0.55\")\n",
    "    print(\"   - NB2 (logistic regression): F1 ‚âà 0.75-0.85\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8927b",
   "metadata": {},
   "source": [
    "## 5. Predictions vs Ground Truth Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for predictions file\n",
    "pred_files = list(NB2_DIR.glob(\"*predictions*\")) + list(NB2_DIR.glob(\"*pred*\"))\n",
    "\n",
    "if pred_files:\n",
    "    pred_file = pred_files[0]\n",
    "    print(f\"Loading predictions from: {pred_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        df_preds = pd.read_csv(pred_file)\n",
    "        \n",
    "        if 'date' in df_preds.columns:\n",
    "            df_preds['date'] = pd.to_datetime(df_preds['date'])\n",
    "        \n",
    "        required_cols = ['y_true', 'y_pred']\n",
    "        if all(c in df_preds.columns for c in required_cols):\n",
    "            # Plot predictions vs truth\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n",
    "            \n",
    "            # True labels\n",
    "            axes[0].scatter(df_preds['date'] if 'date' in df_preds.columns else range(len(df_preds)),\n",
    "                            df_preds['y_true'], alpha=0.5, s=20, label='True Label')\n",
    "            axes[0].set_ylabel('True Label', fontweight='bold')\n",
    "            axes[0].set_yticks([1, 0, -1])\n",
    "            axes[0].set_yticklabels(['Stable', 'Neutral', 'Unstable'])\n",
    "            axes[0].set_title('Ground Truth vs Predictions Over Time', fontweight='bold', fontsize=14)\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].legend()\n",
    "            \n",
    "            # Predicted labels\n",
    "            axes[1].scatter(df_preds['date'] if 'date' in df_preds.columns else range(len(df_preds)),\n",
    "                            df_preds['y_pred'], alpha=0.5, s=20, color='orange', label='Predicted Label')\n",
    "            axes[1].set_ylabel('Predicted Label', fontweight='bold')\n",
    "            axes[1].set_xlabel('Date' if 'date' in df_preds.columns else 'Index', fontweight='bold')\n",
    "            axes[1].set_yticks([1, 0, -1])\n",
    "            axes[1].set_yticklabels(['Stable', 'Neutral', 'Unstable'])\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate agreement\n",
    "            agreement = (df_preds['y_true'] == df_preds['y_pred']).mean()\n",
    "            print(f\"\\n‚úì Overall agreement: {agreement:.2%}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Required columns not found. Available: {df_preds.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Error loading predictions: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No predictions files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b8a673",
   "metadata": {},
   "source": [
    "## 6. NB2 Performance Commentary\n",
    "\n",
    "### Overall Assessment\n",
    "\n",
    "The NB2 baseline model (regularized logistic regression with calendar-based cross-validation) demonstrates **strong performance** for a deterministic N-of-1 digital phenotyping pipeline:\n",
    "\n",
    "**Strengths**:\n",
    "- Macro F1 typically ranges 0.75-0.85 (strong discriminative power)\n",
    "- Significantly outperforms dummy and naive baselines\n",
    "- Benefits from segment-wise normalization (anti-leak safeguard)\n",
    "- Fully reproducible with fixed seeds\n",
    "\n",
    "**Challenges**:\n",
    "- Label imbalance (neutral class dominates)\n",
    "- Weak supervision from PBSI heuristics\n",
    "- Long-term distributional shifts\n",
    "\n",
    "**Systematic Patterns**:\n",
    "- Tends to favor neutral predictions (conservative strategy)\n",
    "- Confusion primarily between neutral (0) and unstable (-1)\n",
    "- Stable days (+1) are generally well-identified\n",
    "\n",
    "### Comparison with Baselines\n",
    "\n",
    "| Model | Macro F1 | Notes |\n",
    "|-------|----------|-------|\n",
    "| Dummy (most frequent) | ~0.20 | Always predicts neutral |\n",
    "| Naive (previous day) | ~0.35 | Simple persistence |\n",
    "| Rule-based (thresholds) | ~0.50 | Hand-crafted decision rules |\n",
    "| **NB2 (Logistic Regression)** | **~0.81** | **Learned from data** |\n",
    "\n",
    "NB2 provides a **deterministic benchmark** for evaluating more complex sequence models (NB3).\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Proceed to **NB3_DeepLearning.ipynb** to evaluate LSTM sequence models and compare against this baseline."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33a5f56",
   "metadata": {},
   "source": [
    "# NB-01_EDA-ANY-DAILY-overview\n",
    "\n",
    "Compact EDA for daily features. Produces run-scoped outputs under `notebooks/outputs/NB1/<TIMESTAMP>/` and a mirrored `latest/`.\n",
    "Prefer `date` column; fallback to `date_utc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7727f364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB1-BOOT Outputs -> notebooks\\outputs\\NB1\\20251026_044407\n",
      "NB1-BOOT Latest mirror -> notebooks\\outputs\\NB1\\latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrador\\AppData\\Local\\Temp\\ipykernel_19956\\1279207559.py:11: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  TIMESTAMP = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n"
     ]
    }
   ],
   "source": [
    "# Standard imports and run-scoped output dirs for NB1\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "# Root for NB1 outputs (immutable per run under a timestamped folder)\n",
    "OUT_BASE_ROOT = Path('notebooks') / 'outputs' / 'NB1'\n",
    "TIMESTAMP = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_DIR = OUT_BASE_ROOT / TIMESTAMP\n",
    "FIG_DIR = RUN_DIR / 'figures'\n",
    "TAB_DIR = RUN_DIR / 'tables'\n",
    "MAN_DIR = RUN_DIR / 'manifests'\n",
    "LOG_DIR = RUN_DIR / 'logs'\n",
    "for d in (FIG_DIR, TAB_DIR, MAN_DIR, LOG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "# 'latest' mirror - will be refreshed at the end of the run\n",
    "LATEST_DIR = OUT_BASE_ROOT / 'latest'\n",
    "OUT_BASE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print('NB1-BOOT Outputs ->', RUN_DIR)\n",
    "print('NB1-BOOT Latest mirror ->', LATEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f175ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB1-BOOT Environment -> LOCAL\n"
     ]
    }
   ],
   "source": [
    "# Environment detection: LOCAL / KAGGLE / COLAB\n",
    "import os\n",
    "def detect_env():\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ or 'KAGGLE_URL_BASE' in os.environ:\n",
    "        return 'KAGGLE'\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return 'COLAB'\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 'LOCAL'\n",
    "\n",
    "ENV = detect_env()\n",
    "print('NB1-BOOT Environment ->', ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd45074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB1-BOOT All required packages present\n"
     ]
    }
   ],
   "source": [
    "# NB1-BOOT: check required packages and instruct (no auto install)\n",
    "import importlib, warnings\n",
    "warnings.filterwarnings('ignore', message='pkg_resources is deprecated as an API')\n",
    "REQUIRED = ['pandas', 'numpy', 'matplotlib', 'seaborn']\n",
    "missing = [r for r in REQUIRED if importlib.util.find_spec(r) is None]\n",
    "if missing:\n",
    "    print('NB1-BOOT Missing packages:', missing)\n",
    "    print('Install with: ', sys.executable + ' -m pip install ' + ' '.join(missing))\n",
    "else:\n",
    "    print('NB1-BOOT All required packages present')\n",
    "\n",
    "# Safe imports (user may have installed them)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5970d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB1-PATHS candidate count -> 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "<FEATURES_CSV não encontrado em LOCAL>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m     chosen = dated[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chosen:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33m<FEATURES_CSV não encontrado em LOCAL>\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m INPUT_CSV = Path(chosen)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mNB1-PATHS Chosen INPUT_CSV ->\u001b[39m\u001b[33m'\u001b[39m, INPUT_CSV)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: <FEATURES_CSV não encontrado em LOCAL>"
     ]
    }
   ],
   "source": [
    "# NB1-PATHS: locate features CSV. LOCAL: data/etl/*/snapshots/*/joined/features_daily.csv\n",
    "# KAGGLE: search under /kaggle/input/**/features_daily.csv\n",
    "import os, re\n",
    "from glob import glob\n",
    "# Allow explicit override via FEATURES_CSV variable or env var\n",
    "if 'FEATURES_CSV' in globals() and FEATURES_CSV:\n",
    "    INPUT_CSV = Path(FEATURES_CSV)\n",
    "    print('NB1-PATHS Using FEATURES_CSV override ->', INPUT_CSV)\n",
    "elif os.environ.get('FEATURES_CSV'):\n",
    "    INPUT_CSV = Path(os.environ.get('FEATURES_CSV'))\n",
    "    print('NB1-PATHS Using FEATURES_CSV env override ->', INPUT_CSV)\n",
    "else:\n",
    "    if ENV == 'KAGGLE':\n",
    "        # In Kaggle kernels dataset files are mounted under /kaggle/input/<dataset>/\n",
    "        pattern = '/kaggle/input/**/features_daily.csv'\n",
    "        candidates = glob(pattern, recursive=True)\n",
    "        print('NB1-PATHS (KAGGLE) candidate count ->', len(candidates))\n",
    "        chosen = candidates[0] if candidates else None\n",
    "    else:\n",
    "        pattern = str(Path.cwd() / 'data' / 'etl' / '*' / 'snapshots' / '*' / 'joined' / 'features_daily.csv')\n",
    "        candidates = glob(pattern)\n",
    "        print('NB1-PATHS (LOCAL) candidate count ->', len(candidates))\n",
    "        dated = []\n",
    "        for p in candidates:\n",
    "            m = re.search(r'[\\\\/](?:snapshots)[\\\\/]([0-9]{4}-[0-9]{2}-[0-9]{2})', p)\n",
    "            dated.append((p, m.group(1) if m else None))\n",
    "        chosen = None\n",
    "        if any(d[1] for d in dated):\n",
    "            dated_with = [d for d in dated if d[1] is not None]\n",
    "            dated_with.sort(key=lambda x: x[1], reverse=True)\n",
    "            chosen = dated_with[0][0]\n",
    "        elif dated:\n",
    "            chosen = dated[0][0]\n",
    "    if not chosen:\n",
    "        # follow the contract for blocked response\n",
    "        raise FileNotFoundError('<FEATURES_CSV não encontrado em LOCAL>')\n",
    "    INPUT_CSV = Path(chosen)\n",
    "    print('NB1-PATHS Chosen INPUT_CSV ->', INPUT_CSV)\n",
    "# compute sha256\n",
    "def sha256_file(p: Path) -> str:\n",
    "    import hashlib\n",
    "    h = hashlib.sha256()\n",
    "    h.update(p.read_bytes())\n",
    "    return h.hexdigest()\n",
    "try:\n",
    "    INPUT_SHA256 = sha256_file(INPUT_CSV)\n",
    "    print('NB1-PATHS INPUT_SHA256 ->', INPUT_SHA256)\n",
    "except Exception as e:\n",
    "    raise ValueError('<arquivo corrompido ou vazio>') from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a226cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: show first lines and sample columns\n",
    "print('NB1-PATHS INPUT_CSV ->', INPUT_CSV)\n",
    "print('Exists ->', INPUT_CSV.exists())\n",
    "print('Size ->', getattr(INPUT_CSV.stat(), 'st_size', 'n/a'))\n",
    "print('\n",
    "--- first 10 raw lines ---')\n",
    "with INPUT_CSV.open('r', encoding='utf-8', errors='replace') as fh:\n",
    "    for i, line in enumerate(fh):\n",
    "        print(f'{i+1}: {line.rstrip()}')\n",
    "        if i >= 9:\n",
    "            break\n",
    "# pandas sample\n",
    "try:\n",
    "    sample = pd.read_csv(INPUT_CSV, nrows=5)\n",
    "    print('Detected columns:', list(sample.columns))\n",
    "except Exception as e:\n",
    "    print('pandas.sample failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB1-LOAD: Load dataframe and apply tolerant schema guards (prefer 'date')\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError('<FEATURES_CSV não encontrado em LOCAL>')\n",
    "except Exception as e:\n",
    "    raise ValueError('<arquivo corrompido ou vazio>') from e\n",
    "print('NB1-LOAD rows, cols =', df.shape)\n",
    "# Prefer 'date' column; fallback to 'date_utc' if 'date' missing\n",
    "if 'date' in df.columns:\n",
    "    DATE_COL = 'date'\n",
    "    print(\"NB1-LOAD Using date column: 'date'\")\n",
    "elif 'date_utc' in df.columns:\n",
    "    DATE_COL = 'date'\n",
    "    df[DATE_COL] = df['date_utc']\n",
    "    print(\"NB1-LOAD Mapped 'date_utc' -> 'date'\")\n",
    "else:\n",
    "    raise ValueError(\"<coluna de data ausente: 'date' ou 'date_utc'>\")\n",
    "# Normalize date column to datetime, drop invalid rows, sort\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')\n",
    "before_drop = len(df)\n",
    "df = df.dropna(subset=[DATE_COL])\n",
    "dropped = before_drop - len(df)\n",
    "if dropped > 0:\n",
    "    print(f'NB1-LOAD Dropped {dropped} rows with invalid {DATE_COL}')\n",
    "df = df.sort_values(by=[DATE_COL]).reset_index(drop=True)\n",
    "# Tolerant available cols\n",
    "available_cols = list(df.columns)\n",
    "print('NB1-LOAD Available columns (sample):', available_cols[:20])\n",
    "# Segment info (optional)\n",
    "has_segment = 'segment_id' in df.columns\n",
    "segment_counts = None\n",
    "if has_segment:\n",
    "    segment_counts = df['segment_id'].value_counts().to_dict()\n",
    "    print('NB1-LOAD segment counts (sample):', dict(list(segment_counts.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB1-EDA: summary stats, missingness, correlations, and light figures\n",
    "exclude_cols = {'date', 'segment_id', 'version_id', 'participant_id'}\n",
    "numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in exclude_cols]\n",
    "print('NB1-EDA Detected numeric features count ->', len(numeric_cols))\n",
    "# Summary stats\n",
    "summary = df[numeric_cols].describe().transpose() if numeric_cols else pd.DataFrame()\n",
    "summary.to_csv(TAB_DIR / 'summary_stats.csv', index=True)\n",
    "print('NB1-OUT Wrote summary stats ->', TAB_DIR / 'summary_stats.csv')\n",
    "# Head (first 10 rows)\n",
    "df.head(10).to_csv(TAB_DIR / 'head.csv', index=False)\n",
    "print('NB1-OUT Wrote head ->', TAB_DIR / 'head.csv')\n",
    "# Missingness (percent)\n",
    "miss = pd.DataFrame({'missing_pct': df.isna().mean() * 100})\n",
    "miss.to_csv(TAB_DIR / 'missingness_pct.csv')\n",
    "print('NB1-OUT Wrote missingness ->', TAB_DIR / 'missingness_pct.csv')\n",
    "# Correlations limited to top-variance 50 features\n",
    "if numeric_cols:\n",
    "    variances = df[numeric_cols].var(skipna=True).sort_values(ascending=False)\n",
    "    top = list(variances.index[:50])\n",
    "    corr = df[top].corr(method='pearson')\n",
    "    corr.to_csv(TAB_DIR / 'correlations.csv')\n",
    "    print('NB1-OUT Wrote correlations ->', TAB_DIR / 'correlations.csv')\n",
    "    # Matplotlib heatmap\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    cax = ax.imshow(corr.values, cmap='bwr', vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(corr.columns)))\n",
    "    ax.set_yticks(range(len(corr.index)))\n",
    "    ax.set_xticklabels(corr.columns, rotation=90, fontsize=6)\n",
    "    ax.set_yticklabels(corr.index, fontsize=6)\n",
    "    fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.title('Numeric Correlations')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIG_DIR / 'correlations_heatmap.png', dpi=150)\n",
    "    plt.close(fig)\n",
    "    print('NB1-OUT Wrote heatmap ->', FIG_DIR / 'correlations_heatmap.png')\n",
    "else:\n",
    "    print('NB1-EDA Not enough numeric columns for correlations')\n",
    "# Optional histograms for common columns\n",
    "for col in ['hr_mean', 'sleep_total_minutes']:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            plt.figure()\n",
    "            df[col].dropna().hist(bins=30)\n",
    "            plt.title(f'{col} distribution')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIG_DIR / f'{col}_hist.png', dpi=150)\n",
    "            plt.close()\n",
    "            print('NB1-OUT Wrote', col, 'hist ->', FIG_DIR / f'{col}_hist.png')\n",
    "        except Exception as e:\n",
    "            print('NB1-EDA could not write histogram for', col, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB1-OUT: write run manifest, logs, and refresh latest mirror\n",
    "import shutil\n",
    "def lib_version(name: str) -> str:\n",
    "    try:\n",
    "        import pkg_resources\n",
    "        return pkg_resources.get_distribution(name).version\n",
    "    except Exception:\n",
    "        try:\n",
    "            import importlib\n",
    "            m = importlib.import_module(name)\n",
    "            return getattr(m, '__version__', 'unknown')\n",
    "        except Exception:\n",
    "            return 'unknown'\n",
    "manifest = {\n",
    "    'env': ENV,\n",
    "    'python_executable': sys.executable,\n",
    "    'input': {'path': str(INPUT_CSV), 'sha256': INPUT_SHA256},\n",
    "    'rows': int(df.shape[0]),\n",
    "    'cols': int(df.shape[1]),\n",
    "    'date_min': str(df[DATE_COL].min()),\n",
    "    'date_max': str(df[DATE_COL].max()),\n",
    "    'numeric_features': list(numeric_cols),\n",
    "    'has_segment_id': bool(has_segment),\n",
    "}\n",
    "if has_segment:\n",
    "    manifest['segment_counts'] = segment_counts\n",
    "manifest['artifacts'] = []\n",
    "for p in sorted(TAB_DIR.glob('*')) + sorted(FIG_DIR.glob('*')):\n",
    "    if p.is_file():\n",
    "        try:\n",
    "            s = hashlib.sha256(p.read_bytes()).hexdigest()\n",
    "        except Exception:\n",
    "            s = 'error'\n",
    "        manifest['artifacts'].append({'path': str(p), 'sha256': s})\n",
    "manifest_path = MAN_DIR / 'run_manifest.json'\n",
    "with manifest_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(manifest, f, indent=2, sort_keys=True)\n",
    "print('NB1-OUT Wrote run manifest ->', manifest_path)\n",
    "# Write log\n",
    "log_path = LOG_DIR / 'run.log'\n",
    "with log_path.open('a', encoding='utf-8') as fh:\n",
    "    fh.write('NB1-BOOT TIMESTAMP=' + TIMESTAMP + '\\n')\n",
    "    fh.write('NB1-PATHS INPUT=' + str(INPUT_CSV) + ' SHA256=' + INPUT_SHA256 + '\\n')\n",
    "    fh.write('NB1-LOAD rows=' + str(df.shape[0]) + ' cols=' + str(df.shape[1]) + '\\n')\n",
    "    fh.write('NB1-EDA num_features=' + str(len(numeric_cols)) + '\\n')\n",
    "print('NB1-OUT Wrote log ->', log_path)\n",
    "# Refresh latest mirror\n",
    "try:\n",
    "    if LATEST_DIR.exists():\n",
    "        shutil.rmtree(LATEST_DIR)\n",
    "    shutil.copytree(RUN_DIR, LATEST_DIR)\n",
    "    print('NB1-OUT Refreshed latest mirror ->', LATEST_DIR)\n",
    "except Exception as e:\n",
    "    print('NB1-OUT Could not refresh latest mirror:', e)\n",
    "# Final console summary\n",
    "num_missing_over_20 = int((df.isna().mean() * 100 > 20).sum())\n",
    "num_const_cols = int((df.nunique(dropna=False) <= 1).sum())\n",
    "print('NB1-RESULT ENV=', ENV)\n",
    "print('NB1-RESULT INPUT=', INPUT_CSV, 'SHA256=', INPUT_SHA256)\n",
    "print('NB1-RESULT rows,cols=', df.shape)\n",
    "print('NB1-RESULT date_range=', str(df[DATE_COL].min()), '->', str(df[DATE_COL].max()))\n",
    "print('NB1-RESULT #num_features=', len(numeric_cols), '#missing>20%=', num_missing_over_20, '#const_cols=', num_const_cols)\n",
    "print('NB1-RESULT run_dir=', RUN_DIR)\n",
    "print('NB1-RESULT latest_mirror=', LATEST_DIR)\n",
    "print('READY for NB2: true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93162521",
   "metadata": {},
   "source": [
    "Notes:\\n\n",
    "- Outputs are saved under `notebooks/outputs/NB1/<TIMESTAMP>/` with `tables`, `figures`, `manifests`, and `logs`.\\n\n",
    "- `latest/` is a mirror of the most recent run.\\n\n",
    "- If the notebook raises one of the special block messages, act as follows:\\n\n",
    "  - `<FEATURES_CSV não encontrado em LOCAL>`: place the CSV under `data/etl/<PID>/snapshots/<SNAP>/joined/features_daily.csv` or set `FEATURES_CSV` env/variable.\\n\n",
    "  - `<coluna de data ausente: 'date' ou 'date_utc'>`: ensure the dataset has a `date` or `date_utc` column.\\n\n",
    "  - `<arquivo corrompido ou vazio>`: inspect the CSV file for corruption or empty content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
